{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyONxfWgnyk0BLfezdz9AW7g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alsoway7/SLM/blob/main/SLM2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86a360db"
      },
      "source": [
        "# Task\n",
        "Load the \"`HuggingFaceH4/ultrachat_200k`\" and \"`daily_dialog`\" datasets, standardize them into a unified conversation format, merge them, and tokenize the combined data using the GPT-2 tokenizer for model training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "044ffd5b"
      },
      "source": [
        "## 集成多源数据集\n",
        "\n",
        "### Subtask:\n",
        "Load the \"HuggingFaceH4/ultrachat_200k\" and \"daily_dialog\" datasets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7ddb550"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the \"HuggingFaceH4/ultrachat_200k\" (train_sft split) and \"daily_dialog\" datasets using `load_dataset`, and print the first entry of each to understand their structure.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d6e87f9"
      },
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the 'train_sft' split of the ultrachat_200k dataset\n",
        "ultrachat_ds = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"train_sft\")\n",
        "\n",
        "# Load the daily_dialog dataset (returns a DatasetDict)\n",
        "dailydialog_ds = load_dataset(\"daily_dialog\")\n",
        "\n",
        "# Print the first example from both datasets to inspect structure\n",
        "print(\"UltraChat 200k Example:\")\n",
        "print(ultrachat_ds[0])\n",
        "print(\"\\nDaily Dialog Example (from train split):\")\n",
        "print(dailydialog_ds['train'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "869a13fd"
      },
      "source": [
        "**Reasoning**:\n",
        "The error `RuntimeError: Dataset scripts are no longer supported` occurs because the `daily_dialog` dataset relies on a Python loading script. To allow this, I will add `trust_remote_code=True` to the `load_dataset` function call.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe3b96ed"
      },
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the 'train_sft' split of the ultrachat_200k dataset\n",
        "ultrachat_ds = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"train_sft\")\n",
        "\n",
        "# Load the daily_dialog dataset (returns a DatasetDict)\n",
        "# trust_remote_code=True is required because this dataset uses a python loading script\n",
        "dailydialog_ds = load_dataset(\"daily_dialog\", trust_remote_code=True)\n",
        "\n",
        "# Print the first example from both datasets to inspect structure\n",
        "print(\"UltraChat 200k Example:\")\n",
        "print(ultrachat_ds[0])\n",
        "print(\"\\nDaily Dialog Example (from train split):\")\n",
        "print(dailydialog_ds['train'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93bf3cda"
      },
      "source": [
        "**Reasoning**:\n",
        "The error `RuntimeError: Dataset scripts are no longer supported` indicates that the `daily_dialog` dataset's Python loading script is blocked by the installed `datasets` library version. To bypass this, I will load the dataset from the `refs/convert/parquet` revision, which points to the auto-converted Parquet files maintained by Hugging Face, thereby avoiding the script execution.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95164ecb"
      },
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the 'train_sft' split of the ultrachat_200k dataset\n",
        "ultrachat_ds = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"train_sft\")\n",
        "\n",
        "# Load the daily_dialog dataset using the parquet revision to avoid script execution issues\n",
        "dailydialog_ds = load_dataset(\"daily_dialog\", revision=\"refs/convert/parquet\")\n",
        "\n",
        "# Print the first example from both datasets to inspect structure\n",
        "print(\"UltraChat 200k Example:\")\n",
        "print(ultrachat_ds[0])\n",
        "print(\"\\nDaily Dialog Example (from train split):\")\n",
        "print(dailydialog_ds['train'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec1a2eb4"
      },
      "source": [
        "## 数据统一与合并\n",
        "\n",
        "### Subtask:\n",
        "Standardize the Daily Dialog dataset to match the UltraChat format and merge both into a single training dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a87bb1b"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a formatting function to transform `daily_dialog` data into the `messages` format (alternating user/assistant roles), apply it to the dataset, select the `messages` column from both datasets to ensure schema consistency, merge them using `concatenate_datasets`, and finally shuffle and inspect the result.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22617082"
      },
      "source": [
        "from datasets import concatenate_datasets\n",
        "\n",
        "# Define function to format daily_dialog to match UltraChat's messages format\n",
        "def format_dailydialog(example):\n",
        "    messages = []\n",
        "    for i, turn in enumerate(example['dialog']):\n",
        "        # Alternate roles: even indices are 'user', odd are 'assistant'\n",
        "        role = 'user' if i % 2 == 0 else 'assistant'\n",
        "        messages.append({'role': role, 'content': turn})\n",
        "    return {'messages': messages}\n",
        "\n",
        "# Apply the formatting to the 'train' split of daily_dialog\n",
        "processed_dailydialog = dailydialog_ds['train'].map(format_dailydialog)\n",
        "\n",
        "# Select only the 'messages' column from both datasets to ensure schema consistency for merging\n",
        "ultrachat_simple = ultrachat_ds.select_columns(['messages'])\n",
        "dailydialog_simple = processed_dailydialog.select_columns(['messages'])\n",
        "\n",
        "# Merge the datasets\n",
        "merged_dataset = concatenate_datasets([ultrachat_simple, dailydialog_simple])\n",
        "\n",
        "# Shuffle the merged dataset and print info\n",
        "merged_dataset = merged_dataset.shuffle(seed=42)\n",
        "print(f\"Merged dataset size: {len(merged_dataset)}\")\n",
        "print(\"First entry example:\")\n",
        "print(merged_dataset[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b23c0a5"
      },
      "source": [
        "## 分词与预处理\n",
        "\n",
        "### Subtask:\n",
        "Tokenize the merged dataset using the GPT-2 tokenizer, formatting the conversation history into a single string and preparing inputs for causal language modeling.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bead470"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the GPT-2 tokenizer, setting the pad token to the eos token. Define a preprocessing function that formats the conversation messages into a single string (e.g., 'User: ... \\nAssistant: ...') ending with the eos token, tokenizes them with padding and truncation, and creates labels for causal language modeling. Apply this function to the merged dataset and verify the output keys.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b9f0a32"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the tokenizer for GPT-2\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "# GPT-2 does not have a pad token by default, so we set it to the eos_token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    texts = []\n",
        "    for messages in examples['messages']:\n",
        "        conversation_text = \"\"\n",
        "        for message in messages:\n",
        "            # capitalize role (User/Assistant)\n",
        "            role = message['role'].capitalize()\n",
        "            content = message['content']\n",
        "            conversation_text += f\"{role}: {content}\\n\"\n",
        "        # Append the eos token at the end of the conversation\n",
        "        conversation_text += tokenizer.eos_token\n",
        "        texts.append(conversation_text)\n",
        "\n",
        "    # Tokenize the texts with padding and truncation\n",
        "    model_inputs = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "    # For Causal Language Modeling (CLM), labels are usually the same as input_ids\n",
        "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
        "    return model_inputs\n",
        "\n",
        "# Apply the preprocessing function to the merged dataset\n",
        "tokenized_dataset = merged_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Verify the processing by printing the keys of the first item\n",
        "print(\"Keys in the processed dataset:\", tokenized_dataset[0].keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c31a3bd4"
      },
      "source": [
        "## Model training\n",
        "\n",
        "### Subtask:\n",
        "Configure and execute the fine-tuning of the GPT-2 model on the tokenized dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb78cb72"
      },
      "source": [
        "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "import torch\n",
        "\n",
        "# Load the pre-trained GPT-2 model\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Initialize the data collator\n",
        "# mlm=False because we are doing Causal Language Modeling\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# Determine if GPU is available for fp16\n",
        "use_fp16 = torch.cuda.is_available()\n",
        "\n",
        "# Define training arguments\n",
        "# Limiting max_steps to 500 to ensure the training completes in a reasonable time for this environment\n",
        "# Removed 'overwrite_output_dir' as it caused a TypeError in previous attempts\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2_finetuned\",\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    max_steps=500,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    fp16=use_fp16,\n",
        "    logging_steps=50,\n",
        "    save_steps=500,\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "# Removed 'tokenizer' argument as it caused a TypeError\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Run the training\n",
        "trainer.train()\n",
        "\n",
        "# Save the model and tokenizer explicitly since tokenizer was removed from Trainer args\n",
        "trainer.save_model(\"./gpt2_finetuned\")\n",
        "tokenizer.save_pretrained(\"./gpt2_finetuned\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47b80627"
      },
      "source": [
        "# Task\n",
        "Load the initially trained model and tokenizer from the \"./gpt2_finetuned\" directory. Configure a second fine-tuning phase with `TrainingArguments` using a lower learning rate (e.g., 1e-5) to refine the model weights. Execute this fine-tuning using the `Trainer` and save the final model to the \"./gpt2_final_finetuned\" directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96a8ac94"
      },
      "source": [
        "## 加载初步训练模型\n",
        "\n",
        "### Subtask:\n",
        "Load the pre-trained model and tokenizer from the `./gpt2_finetuned` directory to serve as the base for the second fine-tuning phase.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c45b78a6"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the fine-tuned model and tokenizer from the local directory `./gpt2_finetuned` to prepare for the next stage of training or inference.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "832cc2ef"
      },
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load the tokenizer from the saved directory\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./gpt2_finetuned\")\n",
        "\n",
        "# Explicitly set the pad token to the eos token for GPT-2\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load the fine-tuned causal language model from the saved directory\n",
        "model = AutoModelForCausalLM.from_pretrained(\"./gpt2_finetuned\")\n",
        "\n",
        "print(\"Model and tokenizer loaded successfully from ./gpt2_finetuned\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3173c8ea"
      },
      "source": [
        "## Execute model fine-tuning\n",
        "\n",
        "### Subtask:\n",
        "Configure the training arguments, initialize the Trainer, execute the fine-tuning on the dataset, and save the final model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8339a483"
      },
      "source": [
        "**Reasoning**:\n",
        "Configure the training arguments with a lower learning rate and specified steps, initialize the Trainer with the model and data collator, execute the fine-tuning, and save the final model and tokenizer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c046f0ec"
      },
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "import torch\n",
        "\n",
        "# Check if CUDA is available for fp16\n",
        "use_fp16 = torch.cuda.is_available()\n",
        "\n",
        "# Define training arguments for the second fine-tuning phase\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./gpt2_final_finetuned',\n",
        "    learning_rate=1e-5,\n",
        "    max_steps=50,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    fp16=use_fp16,\n",
        "    logging_steps=10,\n",
        "    save_steps=50,\n",
        ")\n",
        "\n",
        "# Initialize the data collator\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Execute the fine-tuning\n",
        "trainer.train()\n",
        "\n",
        "# Save the final fine-tuned model and tokenizer\n",
        "trainer.save_model(\"./gpt2_final_finetuned\")\n",
        "tokenizer.save_pretrained(\"./gpt2_final_finetuned\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8d317a5"
      },
      "source": [
        "## Verify the final model\n",
        "\n",
        "### Subtask:\n",
        "Load the fine-tuned model from the saved directory and generate a sample response to verify functionality.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80a43459"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the fine-tuned model and tokenizer from the './gpt2_final_finetuned' directory, move the model to the appropriate device, and generate a sample response for the input 'User: Hello, how are you?\\n' to verify the model's functionality.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8937347b"
      },
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Load the final fine-tuned tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./gpt2_final_finetuned\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"./gpt2_final_finetuned\")\n",
        "\n",
        "# Ensure pad token is set to eos token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Move model to the correct device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "# Define test input\n",
        "input_text = \"User: Hello, how are you?\\n\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Generate response\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=50,\n",
        "    do_sample=True,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "# Decode and print the output\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"Generated Response:\")\n",
        "print(generated_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfeb91ca"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   Successfully loaded the initially trained model and tokenizer from the `./gpt2_finetuned` directory, ensuring the pad token was correctly set to the EOS token.\n",
        "*   Configured the second fine-tuning phase with specific hyperparameters: a learning rate of 1e-5, a batch size of 4 per device, and a limit of 50 maximum training steps.\n",
        "*   Executed the training process using the `Trainer` API and successfully saved the final model artifacts to the `./gpt2_final_finetuned` directory.\n",
        "*   Verified the functionality of the final model by reloading it from the saved directory and successfully generating text based on the prompt \"User: Hello, how are you?\".\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   While the model is functionally verifying by generating text, quantitative metrics (such as perplexity or BLEU scores) should be calculated to measure the specific performance improvement gained from this second refinement phase.\n",
        "*   The model is now ready for deployment or further testing; ensure that the specific prompt template used during training is consistently applied during inference to maintain output quality.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d10dacf"
      },
      "source": [
        "# Task\n",
        "Mount Google Drive to the Colab environment. Then, create a backup directory named `my_gpt2_models` in your Drive (e.g., `/content/drive/MyDrive/my_gpt2_models`) and copy the local `./gpt2_finetuned` and `./gpt2_final_finetuned` folders into it to ensure the trained models are saved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f59c18ca"
      },
      "source": [
        "## 挂载 Google Drive\n",
        "\n",
        "### Subtask:\n",
        "Mount Google Drive to the Colab environment to access cloud storage.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7422392e"
      },
      "source": [
        "**Reasoning**:\n",
        "Import `drive` from `google.colab` and mount the drive to `/content/drive` as requested.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29cbd056"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e2071cd"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous `drive.mount` attempt failed with a `ValueError`, likely due to state inconsistency or a non-interactive environment. I will update the code to use `force_remount=True` and wrap the mounting and subsequent backup operations (creating directory and copying files) in a `try-except` block to ensure the notebook handles the failure gracefully without crashing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0648f27"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "try:\n",
        "    # Mount Google Drive with force_remount to fix potential mount issues\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "    # Define the backup directory path in Google Drive\n",
        "    backup_dir = '/content/drive/MyDrive/my_gpt2_models'\n",
        "\n",
        "    # Create the backup directory if it doesn't exist\n",
        "    os.makedirs(backup_dir, exist_ok=True)\n",
        "    print(f\"Backup directory ensured at: {backup_dir}\")\n",
        "\n",
        "    # List of local model directories to backup\n",
        "    model_dirs = ['./gpt2_finetuned', './gpt2_final_finetuned']\n",
        "\n",
        "    for model_dir in model_dirs:\n",
        "        if os.path.exists(model_dir):\n",
        "            # Construct the full destination path\n",
        "            dest_path = os.path.join(backup_dir, os.path.basename(model_dir))\n",
        "\n",
        "            # Copy the directory, allowing overwrite if it exists\n",
        "            shutil.copytree(model_dir, dest_path, dirs_exist_ok=True)\n",
        "            print(f\"Successfully backed up {model_dir} to {dest_path}\")\n",
        "        else:\n",
        "            print(f\"Warning: Source directory {model_dir} not found.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during Drive mount or backup: {e}\")\n",
        "    print(\"Backup operation skipped. Please ensure you are authenticated in Colab.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ef2e962"
      },
      "source": [
        "## 交互式推理\n",
        "\n",
        "### Subtask:\n",
        "Create an interactive inference loop to chat with the fine-tuned GPT-2 model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8095fe0c"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement an interactive chat loop using the fine-tuned model. I will load the model and tokenizer, set up a `while True` loop to capture user input, format it to match the training data, generate a response, and print the assistant's reply. I will include a try-except block for `EOFError` to ensure the code exits gracefully in non-interactive environments.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6561c1b"
      },
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model_path = \"./gpt2_final_finetuned\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "print(\"Interactive Chat Mode. Type 'exit' or 'quit' to end conversation.\")\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        # Get user input\n",
        "        user_input = input(\"User: \")\n",
        "\n",
        "        # Check for exit condition\n",
        "        if user_input.strip().lower() in [\"exit\", \"quit\"]:\n",
        "            print(\"Exiting chat.\")\n",
        "            break\n",
        "\n",
        "        # Format the input to match training data\n",
        "        # Pattern: User: <input>\\nAssistant:\n",
        "        prompt = f\"User: {user_input}\\nAssistant:\"\n",
        "\n",
        "        # Tokenize and move to device\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        # Generate response\n",
        "        # using parameters to improve variety\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=50,\n",
        "                do_sample=True,\n",
        "                top_p=0.9,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        # Decode the generated tokens\n",
        "        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Extract the Assistant's part\n",
        "        # The prompt ends with \"Assistant:\", so we split by that and take the last part\n",
        "        if \"Assistant:\" in full_response:\n",
        "            assistant_reply = full_response.split(\"Assistant:\")[-1].strip()\n",
        "        else:\n",
        "            assistant_reply = full_response\n",
        "\n",
        "        print(f\"Assistant: {assistant_reply}\\n\")\n",
        "\n",
        "    except EOFError:\n",
        "        # Handle cases where input() receives no input (e.g., non-interactive execution)\n",
        "        print(\"\\nNon-interactive mode detected. Exiting loop.\")\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0184c1bb"
      },
      "source": [
        "# Task\n",
        "Load the `test_sft` split of the \"HuggingFaceH4/ultrachat_200k\" dataset and apply the same preprocessing and tokenization logic used during training. Then, load the fine-tuned model from \"`./gpt2_final_finetuned`\" and use the `Trainer` to evaluate it on the test dataset. Calculate the Perplexity (PPL) using the formula $PPL = e^{eval\\_loss}$ based on the returned evaluation loss and print the result to verify the model's performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73ed19f7"
      },
      "source": [
        "## Prepare evaluation data\n",
        "\n",
        "### Subtask:\n",
        "Load the `test_sft` split of the UltraChat dataset and preprocess it using the fine-tuned tokenizer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e159434c"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the 'test_sft' split of the UltraChat dataset and preprocess it using the fine-tuned tokenizer from './gpt2_final_finetuned', formatting the data for evaluation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6957160"
      },
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the test split of the dataset\n",
        "test_dataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"test_sft\")\n",
        "\n",
        "# Load the tokenizer from the final fine-tuned model directory\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./gpt2_final_finetuned\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Define the preprocessing function\n",
        "def preprocess_function(examples):\n",
        "    texts = []\n",
        "    for messages in examples['messages']:\n",
        "        conversation_text = \"\"\n",
        "        for message in messages:\n",
        "            # Format: Role: Content\\n\n",
        "            role = message['role'].capitalize()\n",
        "            content = message['content']\n",
        "            conversation_text += f\"{role}: {content}\\n\"\n",
        "        # Append eos token\n",
        "        conversation_text += tokenizer.eos_token\n",
        "        texts.append(conversation_text)\n",
        "\n",
        "    # Tokenize inputs\n",
        "    model_inputs = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=512)\n",
        "    # Set labels for causal language modeling\n",
        "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
        "    return model_inputs\n",
        "\n",
        "# Apply preprocessing to the test dataset\n",
        "tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Print the size of the processed dataset\n",
        "print(f\"Processed test dataset size: {len(tokenized_test_dataset)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b3a3d02"
      },
      "source": [
        "# Task\n",
        "From the preprocessed `tokenized_test_dataset`, select a random subset of 500 samples to expedite the evaluation. Use the `Trainer` to perform an evaluation on this subset, then calculate and print the Perplexity (PPL) of the model using the formula $PPL = e^{eval\\_loss}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2501d8f"
      },
      "source": [
        "## 抽取测试子集与快速计算困惑度\n",
        "\n",
        "### Subtask:\n",
        "Select a random subset of 500 samples from the preprocessed test dataset, perform evaluation to get the loss, and calculate the model's perplexity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63bc35d7"
      },
      "source": [
        "**Reasoning**:\n",
        "Select a random subset of 500 samples from the test dataset to reduce evaluation time, then evaluate the model using the existing trainer to calculate loss and perplexity.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47d06951"
      },
      "source": [
        "import math\n",
        "\n",
        "# Create a small_eval_dataset by shuffling and selecting the first 500 examples\n",
        "small_eval_dataset = tokenized_test_dataset.shuffle(seed=42).select(range(2000))\n",
        "\n",
        "# Evaluate the model on the small dataset using the existing trainer\n",
        "print(\"Starting evaluation on small subset...\")\n",
        "eval_results = trainer.evaluate(eval_dataset=small_eval_dataset)\n",
        "\n",
        "# Extract evaluation loss\n",
        "eval_loss = eval_results[\"eval_loss\"]\n",
        "\n",
        "# Calculate Perplexity\n",
        "perplexity = math.exp(eval_loss)\n",
        "\n",
        "# Print results\n",
        "print(f\"Evaluation Loss: {eval_loss:.4f}\")\n",
        "print(f\"Perplexity: {perplexity:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}