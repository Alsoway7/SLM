{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMBFPy81AS6HzdZMOmSXRBl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alsoway7/SLM/blob/main/SLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d689727a"
      },
      "source": [
        "# Task\n",
        "Develop a small language model (SLM), covering its foundational concepts, data preparation, architecture selection, training environment setup, training process, and evaluation methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22180729"
      },
      "source": [
        "## 理解SLM基础知识\n",
        "\n",
        "### Subtask:\n",
        "介绍SLM的基本概念、工作原理、核心组件以及整个开发生命周期。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b9502ba"
      },
      "source": [
        "```markdown\n",
        "## 理解SLM基础知识\n",
        "\n",
        "### 小型语言模型 (SLM) 概述\n",
        "小型语言模型（Small Language Models, SLM）是相对于大型语言模型（Large Language Models, LLM）而言的，它们在参数量、训练数据规模和计算资源需求上都较小。尽管规模较小，SLM在特定任务和资源受限的环境下展现出独特的优势。\n",
        "\n",
        "#### SLM与LLM的主要区别和优势：\n",
        "*   **规模与效率**：SLM拥有更少的参数，因此训练和推理速度更快，所需的计算资源（GPU内存、CPU）更少，能耗也更低。\n",
        "*   **部署灵活性**：由于资源需求低，SLM更容易部署到边缘设备、移动终端或嵌入式系统中，实现离线运行和低延迟响应。\n",
        "*   **成本效益**：训练和运行SLM的成本远低于LLM，这使得它们对预算有限的组织和个人更具吸引力。\n",
        "*   **特定任务优化**：SLM通常通过在特定数据集上进行微调来执行专业任务，从而在特定领域达到甚至超越LLM的性能，同时减少了不必要的泛化能力。\n",
        "*   **数据隐私**：在本地部署的SLM可以更好地保护用户数据隐私，因为数据无需上传到云端。\n",
        "\n",
        "### SLM的基本工作原理\n",
        "SLM（如大多数现代神经网络模型）通过学习海量文本数据中的模式来理解和生成人类语言。其核心在于将输入的文本序列转换为数值表示，然后通过多层神经网络进行处理，最终输出预测结果（例如下一个词、句子的情感或翻译结果）。这个过程通常涉及以下几个步骤：\n",
        "1.  **分词 (Tokenization)**：将原始文本拆分成更小的单元（如单词、子词或字符），这些单元被称为“token”。\n",
        "2.  **嵌入 (Embedding)**：将每个token转换为一个高维向量（词嵌入），这些向量捕捉了词语的语义信息和上下文关系。\n",
        "3.  **序列处理 (Sequence Processing)**：嵌入向量序列通过神经网络层（通常是Transformer架构）进行处理。这些层能够捕捉输入序列中的长距离依赖关系。\n",
        "4.  **输出层 (Output Layer)**：经过多层处理后，模型会根据任务需求生成最终输出，例如通过softmax层预测下一个词的概率分布。\n",
        "\n",
        "### SLM的核心组件\n",
        "SLM的架构通常基于Transformer模型，其核心组件包括：\n",
        "*   **词嵌入层 (Word Embedding Layer)**：将离散的词汇表中的词映射到连续的向量空间中。这些嵌入向量在训练过程中不断优化，使得语义相似的词在向量空间中距离相近。\n",
        "*   **位置编码 (Positional Encoding)**：由于Transformer模型本身不具备处理序列顺序的能力，位置编码被引入以赋予词嵌入向量位置信息。\n",
        "*   **注意力机制 (Attention Mechanism)**：这是Transformer的核心。它允许模型在处理序列中的每个词时，能够“关注”到输入序列中的其他相关词，从而捕捉长距离依赖关系和上下文信息。自注意力（Self-Attention）是其中最常见的一种。\n",
        "*   **多头注意力 (Multi-Head Attention)**：通过并行运行多个注意力机制，模型可以从不同的“表示子空间”中学习信息，增强模型的捕获复杂模式的能力。\n",
        "*   **前馈网络 (Feed-Forward Network)**：在注意力层之后，每个位置的输出都会通过一个独立的全连接前馈网络进行处理，进一步增强模型的表达能力。\n",
        "*   **层归一化 (Layer Normalization)** 和 **残差连接 (Residual Connections)**：这些技术用于稳定训练过程，帮助梯度更好地传播，并允许构建更深层次的模型。\n",
        "\n",
        "### SLM的开发生命周期\n",
        "SLM的开发是一个迭代过程，通常包括以下阶段：\n",
        "1.  **数据收集 (Data Collection)**：根据目标任务收集相关的文本数据。数据量和质量对模型的性能至关重要。\n",
        "2.  **数据预处理 (Data Preprocessing)**：对收集到的数据进行清洗、分词、标准化和增强等操作，使其适合模型训练。这可能包括去除噪声、处理缺失值、统一格式等。\n",
        "3.  **模型选择与架构设计 (Model Selection & Architecture Design)**：选择合适的SLM基座模型（例如，蒸馏或量化过的LLM，或小型Transformer模型）并进行必要的架构调整。\n",
        "4.  **模型训练 (Model Training)**：使用预处理后的数据对模型进行训练。这可能包括预训练（从头开始或在大型通用语料库上进行）和微调（在特定任务数据集上进行）。\n",
        "5.  **模型评估 (Model Evaluation)**：使用独立的验证集和测试集评估模型的性能。常用的评估指标包括准确率、F1分数、困惑度（Perplexity）等，并根据结果调整模型。\n",
        "6.  **模型优化 (Model Optimization)**：在评估过程中发现性能瓶颈后，可能需要进行模型压缩（如剪枝、量化、知识蒸馏）或超参数调优，以提高效率和性能。\n",
        "7.  **模型部署 (Model Deployment)**：将训练好并优化的模型部署到实际应用环境中，使其能够提供服务。这可能涉及将模型封装为API，或集成到边缘设备中。\n",
        "8.  **监控与维护 (Monitoring & Maintenance)**：部署后持续监控模型的性能，收集用户反馈，并根据需要进行模型更新和再训练，以应对数据漂移或新的业务需求。\n",
        "\n",
        "这个生命周期是一个持续优化的过程，每个阶段都可能需要根据前一阶段的结果进行调整和迭代。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f048109d"
      },
      "source": [
        "```markdown\n",
        "## 理解SLM基础知识\n",
        "\n",
        "### 小型语言模型 (SLM) 概述\n",
        "小型语言模型（Small Language Models, SLM）是相对于大型语言模型（Large Language Models, LLM）而言的，它们在参数量、训练数据规模和计算资源需求上都较小。尽管规模较小，SLM在特定任务和资源受限的环境下展现出独特的优势。\n",
        "\n",
        "#### SLM与LLM的主要区别和优势：\n",
        "*   **规模与效率**：SLM拥有更少的参数，因此训练和推理速度更快，所需的计算资源（GPU内存、CPU）更少，能耗也更低。\n",
        "*   **部署灵活性**：由于资源需求低，SLM更容易部署到边缘设备、移动终端或嵌入式系统中，实现离线运行和低延迟响应。\n",
        "*   **成本效益**：训练和运行SLM的成本远低于LLM，这使得它们对预算有限的组织和个人更具吸引力。\n",
        "*   **特定任务优化**：SLM通常通过在特定数据集上进行微调来执行专业任务，从而在特定领域达到甚至超越LLM的性能，同时减少了不必要的泛化能力。\n",
        "*   **数据隐私**：在本地部署的SLM可以更好地保护用户数据隐私，因为数据无需上传到云端。\n",
        "\n",
        "### SLM的基本工作原理\n",
        "SLM（如大多数现代神经网络模型）通过学习海量文本数据中的模式来理解和生成人类语言。其核心在于将输入的文本序列转换为数值表示，然后通过多层神经网络进行处理，最终输出预测结果（例如下一个词、句子的情感或翻译结果）。这个过程通常涉及以下几个步骤：\n",
        "1.  **分词 (Tokenization)**：将原始文本拆分成更小的单元（如单词、子词或字符），这些单元被称为“token”。\n",
        "2.  **嵌入 (Embedding)**：将每个token转换为一个高维向量（词嵌入），这些向量捕捉了词语的语义信息和上下文关系。\n",
        "3.  **序列处理 (Sequence Processing)**：嵌入向量序列通过神经网络层（通常是Transformer架构）进行处理。这些层能够捕捉输入序列中的长距离依赖关系。\n",
        "4.  **输出层 (Output Layer)**：经过多层处理后，模型会根据任务需求生成最终输出，例如通过softmax层预测下一个词的概率分布。\n",
        "\n",
        "### SLM的核心组件\n",
        "SLM的架构通常基于Transformer模型，其核心组件包括：\n",
        "*   **词嵌入层 (Word Embedding Layer)**：将离散的词汇表中的词映射到连续的向量空间中。这些嵌入向量在训练过程中不断优化，使得语义相似的词在向量空间中距离相近。\n",
        "*   **位置编码 (Positional Encoding)**：由于Transformer模型本身不具备处理序列顺序的能力，位置编码被引入以赋予词嵌入向量位置信息。\n",
        "*   **注意力机制 (Attention Mechanism)**：这是Transformer的核心。它允许模型在处理序列中的每个词时，能够“关注”到输入序列中的其他相关词，从而捕捉长距离依赖关系和上下文信息。自注意力（Self-Attention）是其中最常见的一种。\n",
        "*   **多头注意力 (Multi-Head Attention)**：通过并行运行多个注意力机制，模型可以从不同的“表示子空间”中学习信息，增强模型的捕获复杂模式的能力。\n",
        "*   **前馈网络 (Feed-Forward Network)**：在注意力层之后，每个位置的输出都会通过一个独立的全连接前馈网络进行处理，进一步增强模型的表达能力。\n",
        "*   **层归一化 (Layer Normalization)** 和 **残差连接 (Residual Connections)**：这些技术用于稳定训练过程，帮助梯度更好地传播，并允许构建更深层次的模型。\n",
        "\n",
        "### SLM的开发生命周期\n",
        "SLM的开发是一个迭代过程，通常包括以下阶段：\n",
        "1.  **数据收集 (Data Collection)**：根据目标任务收集相关的文本数据。数据量和质量对模型的性能至关重要。\n",
        "2.  **数据预处理 (Data Preprocessing)**：对收集到的数据进行清洗、分词、标准化和增强等操作，使其适合模型训练。这可能包括去除噪声、处理缺失值、统一格式等。\n",
        "3.  **模型选择与架构设计 (Model Selection & Architecture Design)**：选择合适的SLM基座模型（例如，蒸馏或量化过的LLM，或小型Transformer模型）并进行必要的架构调整。\n",
        "4.  **模型训练 (Model Training)**：使用预处理后的数据对模型进行训练。这可能包括预训练（从头开始或在大型通用语料库上进行）和微调（在特定任务数据集上进行）。\n",
        "5.  **模型评估 (Model Evaluation)**：使用独立的验证集和测试集评估模型的性能。常用的评估指标包括准确率、F1分数、困惑度（Perplexity）等，并根据结果调整模型。\n",
        "6.  **模型优化 (Model Optimization)**：在评估过程中发现性能瓶颈后，可能需要进行模型压缩（如剪枝、量化、知识蒸馏）或超参数调优，以提高效率和性能。\n",
        "7.  **模型部署 (Model Deployment)**：将训练好并优化的模型部署到实际应用环境中，使其能够提供服务。这可能涉及将模型封装为API，或集成到边缘设备中。\n",
        "8.  **监控与维护 (Monitoring & Maintenance)**：部署后持续监控模型的性能，收集用户反馈，并根据需要进行模型更新和再训练，以应对数据漂移或新的业务需求。\n",
        "\n",
        "这个生命周期是一个持续优化的过程，每个阶段都可能需要根据前一阶段的结果进行调整和迭代。\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d124df6"
      },
      "source": [
        "## 数据准备\n",
        "\n",
        "### Subtask:\n",
        "指导您如何收集、清洗和分词（tokenization）适合训练SLM的数据集。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "866358d4"
      },
      "source": [
        "### 1. 确定数据类型和来源\n",
        "\n",
        "训练**小型语言模型 (SLM)** 需要大量高质量的文本数据。数据类型和来源的选择取决于您的具体任务（例如，文本生成、问答、情感分析等）。\n",
        "\n",
        "**常见数据类型包括：**\n",
        "*   **通用文本：** 新闻文章、书籍、维基百科、网络爬取数据等。这类数据有助于模型学习广泛的语言模式和常识。\n",
        "*   **特定领域文本：** 如果您的SLM旨在处理特定领域的任务，例如医疗、金融或法律，则需要收集该领域专业的文本数据（如医学论文、金融报告、法律文档）。\n",
        "*   **对话数据：** 如果您的SLM用于构建聊天机器人或对话系统，则需要对话数据集。\n",
        "\n",
        "**常见数据来源：**\n",
        "*   **公开数据集平台：**\n",
        "    *   **Hugging Face Datasets:** 提供了大量预处理好的数据集，涵盖多种语言和任务。例如：`common_crawl`, `wikitext`, `openwebtext`。\n",
        "    *   **Kaggle:** 拥有丰富的社区贡献数据集，可能包含特定领域的数据。\n",
        "    *   **Google Dataset Search:** 一个强大的搜索引擎，可以帮助您发现全球的数据集。\n",
        "*   **自定义数据收集：**\n",
        "    *   **网络爬虫：** 使用`BeautifulSoup`、`Scrapy`等工具从特定网站抓取数据。\n",
        "    *   **API：** 许多服务（如Twitter、Reddit）提供API接口供用户获取数据。\n",
        "    *   **企业内部数据：** 如果是企业级应用，可能会使用私有数据。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb0cb4f3"
      },
      "source": [
        "### 2. 数据预处理\n",
        "\n",
        "收集到原始文本数据后，需要对其进行清洗和预处理，以确保数据质量并使其适合模型训练。预处理的目标是减少噪声、标准化文本，并提高模型的学习效率和性能。\n",
        "\n",
        "**常见的预处理步骤包括：**\n",
        "\n",
        "*   **去除HTML标签和XML标记：** 如果数据是从网页或结构化文档中抓取的，可能会包含HTML或XML标签。这些标签通常对语言模型没有意义，需要去除。可以使用正则表达式或专门的库（如`BeautifulSoup`）来完成。\n",
        "\n",
        "    ```python\n",
        "    import re\n",
        "    from bs4 import BeautifulSoup\n",
        "\n",
        "    def remove_html_tags(text):\n",
        "        # 使用BeautifulSoup去除HTML标签\n",
        "        soup = BeautifulSoup(text, 'html.parser')\n",
        "        clean_text = soup.get_text()\n",
        "        return clean_text\n",
        "    \n",
        "    # 示例\n",
        "    html_text = \"<html><body><h1>Title</h1><p>This is a paragraph.</p></body></html>\"\n",
        "    cleaned_text = remove_html_tags(html_text)\n",
        "    print(f\"Original: {html_text}\")\n",
        "    print(f\"Cleaned: {cleaned_text}\")\n",
        "    ```\n",
        "\n",
        "*   **去除特殊字符和标点符号：** 根据任务需求，可能需要去除多余的标点符号、数字、表情符号或其他非文本字符。但要注意，在某些任务中（如情感分析），标点符号和表情符号可能包含重要信息。\n",
        "\n",
        "    ```python\n",
        "    def remove_special_characters(text):\n",
        "        # 保留字母、数字和常用标点符号，去除其他特殊字符\n",
        "        # 注意：这里可以根据具体需求调整正则表达式\n",
        "        clean_text = re.sub(r'[^\\w\\s.,?!]', '', text)\n",
        "        return clean_text\n",
        "\n",
        "    # 示例\n",
        "    text_with_special_chars = \"Hello, world! How are you? 😊 This is a test: 123.\"\n",
        "    cleaned_text = remove_special_characters(text_with_special_chars)\n",
        "    print(f\"Original: {text_with_special_chars}\")\n",
        "    print(f\"Cleaned: {cleaned_text}\")\n",
        "    ```\n",
        "\n",
        "*   **统一文本格式（小写转换）：** 将所有文本转换为小写有助于减少词汇量，并使模型将'Word'、'word'和'WORD'视为同一个词。然而，在某些情况下（如命名实体识别），大小写信息可能很重要。\n",
        "\n",
        "    ```python\n",
        "    def to_lowercase(text):\n",
        "        return text.lower()\n",
        "\n",
        "    # 示例\n",
        "    upper_case_text = \"THIS IS A TEST SENTENCE.\"\n",
        "    cleaned_text = to_lowercase(upper_case_text)\n",
        "    print(f\"Original: {upper_case_text}\")\n",
        "    print(f\"Cleaned: {cleaned_text}\")\n",
        "    ```\n",
        "\n",
        "*   **去除重复内容：** 大规模数据集中经常包含重复的句子、段落甚至整个文档。去除重复内容可以防止模型过拟合，并提高训练效率。这可以通过计算文本哈希值或使用更复杂的相似性检测算法来实现。\n",
        "\n",
        "    ```python\n",
        "    def remove_duplicates(texts_list):\n",
        "        unique_texts = []\n",
        "        seen = set()\n",
        "        for text in texts_list:\n",
        "            if text not in seen:\n",
        "                unique_texts.append(text)\n",
        "                seen.add(text)\n",
        "        return unique_texts\n",
        "\n",
        "    # 示例\n",
        "    text_data = [\n",
        "        \"This is a sample sentence.\",\n",
        "        \"Another sentence.\",\n",
        "        \"This is a sample sentence.\",\n",
        "        \"Yet another sentence.\"\n",
        "    ]\n",
        "    cleaned_list = remove_duplicates(text_data)\n",
        "    print(f\"Original list: {text_data}\")\n",
        "    print(f\"Cleaned list: {cleaned_list}\")\n",
        "    ```\n",
        "\n",
        "*   **处理缺失值和非文本数据：** 检查数据中是否存在空值、无效编码或非文本条目，并根据需要进行删除、填充或转换。\n",
        "\n",
        "*   **去除停用词（可选）：** 停用词（如“的”、“是”、“在”）在某些情况下可能提供较少的信息。然而，对于语言模型，停用词通常是上下文的重要组成部分，因此在训练SLM时，通常不建议去除停用词，除非有特殊任务需求。\n",
        "\n",
        "*   **文本规范化：** 例如，将数字转换为特殊标记，处理缩写，或将不同形式的词语（如“run”、“running”、“ran”）标准化为词根。\n",
        "\n",
        "**整合预处理流程：**\n",
        "\n",
        "在实际应用中，您会创建一个管道，将这些预处理步骤按顺序应用于您的数据集。例如，可以先去除HTML标签，然后转换为小写，最后去除特殊字符。\n",
        "\n",
        "```python\n",
        "# 假设有一个文本列表需要清洗\n",
        "raw_data = [\n",
        "    \"<html><p>Hello World!</p></html>\",\n",
        "    \"Another example text with some SPecial char@cters.\",\n",
        "    \"Duplicate sentence.\",\n",
        "    \"Duplicate sentence.\"\n",
        "]\n",
        "\n",
        "# 组合预处理函数\n",
        "def preprocess_text_pipeline(text):\n",
        "    text = remove_html_tags(text)      # 1. 去除HTML\n",
        "    text = to_lowercase(text)          # 2. 转换为小写\n",
        "    text = remove_special_characters(text) # 3. 去除特殊字符 (根据需求调整)\n",
        "    return text\n",
        "\n",
        "# 应用到所有文本\n",
        "cleaned_individual_texts = [preprocess_text_pipeline(t) for t in raw_data]\n",
        "\n",
        "# 最后去除重复项 (可能在单个文本处理后进行，也可能在整个数据集层面进行)\n",
        "final_cleaned_data = remove_duplicates(cleaned_individual_texts)\n",
        "\n",
        "print(\"--- Raw Data ---\")\n",
        "for item in raw_data:\n",
        "    print(item)\n",
        "\n",
        "print(\"\\n--- Preprocessed Data ---\")\n",
        "for item in final_cleaned_data:\n",
        "    print(item)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def to_lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "# 示例\n",
        "upper_case_text = \"THIS IS A TEST SENTENCE.\"\n",
        "cleaned_text = to_lowercase(upper_case_text)\n",
        "print(f\"Original: {upper_case_text}\")\n",
        "print(f\"Cleaned: {cleaned_text}\")"
      ],
      "metadata": {
        "id": "MebwBoayY4pI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tolower(text):\n",
        "  return text.lower()\n",
        "\n",
        "testtext=\"THIS IS SO GOOD\"\n",
        "cleantext=tolower(testtext)\n",
        "print(f\"clean words are {cleantext}\")"
      ],
      "metadata": {
        "id": "NQdQLMNqfBAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58001ba5"
      },
      "source": [
        "### 3. 分词 (Tokenization)\n",
        "\n",
        "分词是将文本分解成更小单元（称为**标记**或**token**）的过程。这些标记是语言模型能够理解和处理的基本输入单位。分词是自然语言处理（NLP）中的一个关键步骤，对于训练SLM尤其重要。\n",
        "\n",
        "**分词的原理和重要性：**\n",
        "\n",
        "*   **模型输入：** 语言模型无法直接处理原始文本字符串。它们需要数字表示，而分词是实现这一点的第一步。每个标记都会被映射到一个唯一的整数ID，然后这些ID可以嵌入到向量空间中。\n",
        "*   **词汇表构建：** 分词器会构建一个词汇表（vocabulary），其中包含模型已知的所有唯一标记。词汇表的大小直接影响模型的复杂性和效率。\n",
        "*   **处理未见过词：** 传统的分词方法（如基于单词的分词）在遇到训练数据中未出现的词时会遇到困难（即“词汇表外”或OOV问题）。现代的**子词分词（Subword Tokenization）**方法，如BPE (Byte Pair Encoding)、WordPiece和Unigram，通过将词分解成更小的、有意义的子单元来解决这个问题。这使得模型能够处理新词、罕见词和拼写错误，同时保持词汇表在一个可控的大小。\n",
        "*   **上下文理解：** 适当的分词有助于模型更好地捕捉单词、短语和句子之间的语义和句法关系。\n",
        "\n",
        "**使用预训练分词器：**\n",
        "\n",
        "现代SLM通常使用预训练的分词器，这些分词器与其相应的预训练模型（如BERT、GPT、T5）一同发布。Hugging Face Transformers库提供了一个便捷的接口`AutoTokenizer`来加载和使用这些分词器。\n",
        "\n",
        "`AutoTokenizer`会根据指定的模型名称自动下载并加载正确的分词器及其词汇表。\n",
        "\n",
        "```python\n",
        "# 确保已安装transformers库\n",
        "# !pip install transformers\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# 假设我们选择一个预训练模型（例如BERT base uncased）\n",
        "# 分词器会与模型相匹配\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# 需要分词的文本\n",
        "text_to_tokenize = \"Tokenization is a crucial step in preparing text data for language models. It helps handle out-of-vocabulary words.\"\n",
        "\n",
        "# 对文本进行分词\n",
        "# `truncation=True` 处理过长文本，`padding=True` 填充至最长序列长度\n",
        "# `return_tensors=\"pt\"` 返回PyTorch tensors，也可以是\"tf\"或\"np\"\n",
        "encoded_input = tokenizer(text_to_tokenize, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "print(f\"Original Text: {text_to_tokenize}\")\n",
        "print(f\"\\nEncoded Input (Tensor): {encoded_input}\")\n",
        "print(f\"\\nToken IDs: {encoded_input['input_ids']}\")\n",
        "print(f\"Attention Mask: {encoded_input['attention_mask']}\")\n",
        "print(f\"Decoded Tokens: {tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][0])}\")\n",
        "print(f\"Decoded Text: {tokenizer.decode(encoded_input['input_ids'][0])}\")\n",
        "\n",
        "# 批量分词示例\n",
        "batch_texts = [\n",
        "    \"Hello, how are you?\",\n",
        "    \"This is another example sentence for batch processing.\"\n",
        "]\n",
        "\n",
        "batch_encoded_input = tokenizer(batch_texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "print(f\"\\nBatch Encoded Input (Tensor): {batch_encoded_input}\")\n",
        "print(f\"\\nBatch Token IDs: {batch_encoded_input['input_ids']}\")\n",
        "```\n",
        "\n",
        "**分词器输出的含义：**\n",
        "\n",
        "*   `input_ids`：这是分词后的标记序列，每个标记被映射成词汇表中的一个唯一整数ID。\n",
        "*   `attention_mask`：这是一个二进制掩码，用于指示哪些是实际的标记（值为1），哪些是填充标记（值为0）。这对于告诉模型在处理填充部分时应忽略它们非常重要。\n",
        "*   `token_type_ids`（可选）：对于某些模型（如BERT），这用于区分输入中的不同部分（例如，在问答任务中区分问题和上下文）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cffdf492"
      },
      "source": [
        "## 模型架构选择\n",
        "\n",
        "### Subtask:\n",
        "建议适合SLM的常见模型架构（例如，较小的Transformer模型），并解释其基本原理。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "661b8876"
      },
      "source": [
        "### 1. 适合SLM的模型架构选择因素\n",
        "\n",
        "选择适合SLM的模型架构时，需要综合考虑多个关键因素，以确保模型在资源受限的环境中仍能高效、准确地完成任务。\n",
        "\n",
        "*   **模型大小 (Model Size)**：SLM的核心特征是其较小的参数量和计算量。因此，首要考虑的是模型的整体大小，包括参数数量和内存占用。更小的模型意味着更快的推理速度和更低的部署成本。\n",
        "\n",
        "*   **效率 (Efficiency)**：这涉及到模型的训练和推理效率。高效的模型架构能够以更少的计算资源（如GPU或CPU时间）达到可接受的性能。这通常通过优化网络结构、减少层数或采用更轻量级的注意力机制来实现。\n",
        "\n",
        "*   **特定任务需求 (Specific Task Requirements)**：不同的任务可能对模型有不同的要求。例如，对于需要细致理解上下文的任务，Transformer的注意力机制可能更为关键；对于实时推理，则可能需要更快的模型。模型的选择应与目标应用场景紧密结合。\n",
        "\n",
        "*   **部署环境 (Deployment Environment)**：模型将在何种设备上运行（例如，边缘设备、移动手机、低功耗服务器）会直接影响其架构选择。例如，某些硬件可能对特定的操作（如量化推理）有更好的支持。\n",
        "\n",
        "*   **预训练数据和任务 (Pre-training Data and Tasks)**：如果模型已经在一个与目标任务相关的、大规模的数据集上进行了预训练，那么它在微调阶段通常会表现得更好，即使其规模较小。\n",
        "\n",
        "*   **可解释性 (Interpretability)**：在某些应用中，理解模型如何做出决策也很重要。某些架构可能比其他架构更易于解释。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5b24783"
      },
      "source": [
        "### 2. Transformer架构作为SLM的通用基础\n",
        "\n",
        "Transformer模型自2017年被提出以来，已成为自然语言处理（NLP）领域的基石，也是大多数SLM的基础架构。其核心思想是“注意力机制”（Attention Mechanism），它允许模型在处理序列数据时，动态地权衡不同部分的关联性。这使得Transformer能够并行处理数据，并且更好地捕捉长距离依赖关系，解决了传统循环神经网络（RNN）和卷积神经网络（CNN）在处理长序列时的局限性。\n",
        "\n",
        "#### Transformer的基本构成\n",
        "\n",
        "一个完整的Transformer模型通常由两个主要部分组成：\n",
        "\n",
        "1.  **编码器（Encoder）**：负责将输入序列（如文本）转换成一系列的上下文向量表示。每个编码器层包含两个子层：一个多头自注意力机制（Multi-Head Self-Attention）和一个前馈神经网络（Feed-Forward Network）。自注意力机制允许编码器在处理每个单词时，能够关注输入序列中的所有其他单词，并赋予它们不同的权重。这使得模型能够理解单词在不同语境下的含义。\n",
        "\n",
        "2.  **解码器（Decoder）**：负责根据编码器输出的上下文向量和之前生成的输出序列，逐步生成新的输出序列。每个解码器层包含三个子层：一个多头自注意力机制（用于处理解码器自身的输出）、一个多头交叉注意力机制（用于关注编码器的输出）和一个前馈神经网络。解码器中的自注意力通常是带掩码的（masked），以防止在生成当前词时“看到”未来的词。\n",
        "\n",
        "#### 适用于SLM的Transformer变体\n",
        "\n",
        "鉴于SLM对资源效率的要求，通常会采用Transformer的简化或特定变体：\n",
        "\n",
        "*   **仅编码器（Encoder-Only）架构**：这类模型（如BERT、RoBERTa、DistilBERT）主要用于理解和编码输入文本，在诸如文本分类、命名实体识别、问答等任务中表现出色。它们不包含传统的解码器部分，而是将编码器输出的表示用于下游任务的预测层。这种架构相对简单，计算成本较低，非常适合作为SLM的基础。\n",
        "\n",
        "*   **仅解码器（Decoder-Only）架构**：这类模型（如GPT系列、GPT-2 small）主要用于文本生成任务。它们只包含解码器层，通过预测序列中的下一个词来生成连贯的文本。由于其生成能力，这类模型在聊天机器人、内容创作和代码生成等任务中非常有用。对于需要生成连贯长文本的SLM，这种架构是优选。\n",
        "\n",
        "*   **编码器-解码器（Encoder-Decoder）架构**：虽然标准的编码器-解码器Transformer模型（如T5、BART）通常较大，但其小规模版本或经过裁剪的版本也可以用于SLM，尤其是在机器翻译、文本摘要等需要将一种序列转换为另一种序列的任务中。它们能够更好地处理序列到序列（Seq2Seq）的任务，但通常比仅编码器或仅解码器模型需要更多的计算资源。\n",
        "\n",
        "**基本原理总结**：Transformer通过其独特的自注意力机制，能够高效地处理序列数据，捕捉上下文依赖关系。通过选择仅编码器或仅解码器等简化架构，可以在保持核心优势的同时，大幅减少模型参数和计算量，使其适用于资源受限的SLM场景。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1dd02a0"
      },
      "source": [
        "### 3. 具体的SLM模型架构建议\n",
        "\n",
        "针对SLM的需求，以下是一些常见的、适合在资源受限环境下部署的Transformer变体模型架构：\n",
        "\n",
        "*   **DistilBERT**：\n",
        "    *   **特点**：DistilBERT 是 BERT 的一个“蒸馏”（distilled）版本，通过知识蒸馏技术，在保持大部分性能的同时，显著减少了模型大小和计算量。它的层数更少，但通过从大型 BERT 模型中学习，依然能够获得强大的语言理解能力。\n",
        "    *   **优势**：参数量减少了40%，速度提升了60%，但性能仅下降了不到3%。这使得它非常适合需要高性能但资源有限的应用场景，如移动设备或边缘计算。\n",
        "\n",
        "*   **BERT-tiny / TinyBERT**：\n",
        "    *   **特点**：TinyBERT 采用了一种更复杂的蒸馏方法，包括通用蒸馏和特定任务蒸馏，来缩小 BERT 模型。它能够将 BERT-base 压缩到很小的规模，同时尽可能地保留其性能。\n",
        "    *   **优势**：进一步缩小了模型体积，使其在极度资源受限的环境中也能运行，例如物联网设备或需要极低延迟的场景。例如，BERT-tiny可能只有4层，远小于BERT-base的12层。\n",
        "\n",
        "*   **MobileBERT**：\n",
        "    *   **特点**：MobileBERT 是一种专门为移动设备设计的 BERT 变体，它通过结合模型蒸馏和结构优化（如使用瓶颈结构减少参数和计算量）来提高效率。它的设计目标是在移动设备上达到与 BERT-large 相当的性能。\n",
        "    *   **优势**：在保持较高准确性的同时，模型体积更小，推理速度更快，是移动应用中部署大型语言模型的理想选择。\n",
        "\n",
        "*   **GPT-2 small / GPT-Neo / GPT-J (较小版本)**：\n",
        "    *   **特点**：这些是基于仅解码器（Decoder-Only）Transformer 架构的模型，主要用于文本生成任务。GPT-2 small 是 GPT-2 系列中最小的版本。GPT-Neo 和 GPT-J 是 EleutherAI 社区开发的开源替代品，提供各种规模，包括与 GPT-2 small 相当或更小的版本。\n",
        "    *   **优势**：虽然这些模型专注于生成，但它们的较小版本在资源允许的情况下，仍能提供合理的文本生成质量，适用于简单的对话系统、内容辅助创作等。例如，GPT-2 small 拥有1.24亿参数，相较于其更大版本显著减小。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b79a2933"
      },
      "source": [
        "### 4. 模型压缩技术在SLM适配中的作用\n",
        "\n",
        "为了将大型预训练模型（如BERT、GPT-3等）适配为可以在资源受限环境中运行的SLM，模型压缩技术至关重要。这些技术旨在减少模型的参数数量和计算复杂度，同时尽可能保持其性能。常见的模型压缩技术包括：\n",
        "\n",
        "*   **知识蒸馏 (Knowledge Distillation)**：\n",
        "    *   **原理**：这是一种“教师-学生”学习范式。一个大型的、高性能的“教师”模型（Teacher Model）将其学到的知识传递给一个小型、高效的“学生”模型（Student Model）。学生模型不仅学习训练数据的硬标签（hard labels），还学习教师模型预测的软标签（soft labels，即预测的概率分布），从而模仿教师模型的行为。\n",
        "    *   **作用**：通过知识蒸馏，学生模型能够以更小的规模获得接近教师模型的性能，大大减少推理时间并降低部署成本。DistilBERT和TinyBERT就是通过知识蒸馏技术产生的典型SLM。\n",
        "\n",
        "*   **模型剪枝 (Model Pruning)**：\n",
        "    *   **原理**：剪枝是指移除模型中不重要或冗余的连接、神经元或权重，从而减少模型的参数数量和计算量。通常通过设定一个阈值来判断权重的重要性，然后将低于阈值的权重置零或直接移除。剪枝可以是非结构化的（移除单个权重）或结构化的（移除整个神经元、层或通道）。\n",
        "    *   **作用**：剪枝可以显著减少模型大小，降低内存占用和计算需求，从而提高模型的推理速度。经过剪枝的模型更适合部署在内存和计算能力有限的设备上。\n",
        "\n",
        "*   **量化 (Quantization)**：\n",
        "    *   **原理**：量化是指将模型的权重和激活值从高精度表示（如32位浮点数）转换为低精度表示（如8位或4位整数）。较低的精度可以显著减少模型的大小和计算时的内存带宽需求，同时利用硬件对低精度运算的优化。\n",
        "    *   **作用**：量化可以直接减少模型存储空间和推理延迟，尤其适用于移动设备和嵌入式系统。它可以分为训练后量化（Post-Training Quantization, PTQ）和量化感知训练（Quantization Aware Training, QAT），后者通常能获得更好的精度保持。\n",
        "\n",
        "**综合应用**：在实际应用中，这些技术通常可以结合使用，例如先进行知识蒸馏得到一个较小的学生模型，然后对学生模型进行剪枝和量化，以实现极致的模型压缩和效率提升。这些技术是构建高效SLM的关键手段。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2ee49a9"
      },
      "source": [
        "### 5. 在Hugging Face Transformers库中加载和使用SLM\n",
        "\n",
        "Hugging Face Transformers库是一个功能强大且广泛使用的工具，它提供了大量预训练模型，并支持便捷地加载、使用和微调这些模型，包括许多适合SLM的变体。以下是加载和使用这些模型的基本步骤和要点：\n",
        "\n",
        "1.  **安装库**：\n",
        "\n",
        "    首先，确保已经安装了`transformers`库：\n",
        "    ```bash\n",
        "    !pip install transformers\n",
        "    ```\n",
        "\n",
        "2.  **加载Tokenize和模型**：\n",
        "\n",
        "    对于大多数SLM，加载过程非常相似。你需要一个分词器（Tokenizer）来将文本转换为模型可以理解的数值ID，以及模型本身。\n",
        "\n",
        "    *   **对于BERT-like（仅编码器）模型（例如DistilBERT、BERT-tiny）**：\n",
        "        ```python\n",
        "        from transformers import AutoTokenizer, AutoModelForSequenceClassification # 根据任务选择合适的Model类\n",
        "\n",
        "        model_name = \"distilbert-base-uncased\" # 或者 \"prajjwal1/bert-tiny\", \"google/mobilebert-uncased\"\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "        print(f\"成功加载模型：{model_name}\")\n",
        "        ```\n",
        "\n",
        "    *   **对于GPT-like（仅解码器）模型（例如GPT-2 small、GPT-Neo）**：\n",
        "        ```python\n",
        "        from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "        model_name = \"gpt2\" # 这是GPT-2 small版本, 或者 \"EleutherAI/gpt-neo-125M\"\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "        print(f\"成功加载模型：{model_name}\")\n",
        "        ```\n",
        "\n",
        "3.  **模型推理（Inference）**：\n",
        "\n",
        "    一旦模型被加载，就可以用它进行推理。以下是常见任务的示例：\n",
        "\n",
        "    *   **文本分类（使用仅编码器模型）**：\n",
        "        ```python\n",
        "        import torch\n",
        "\n",
        "        text = \"This is a great movie!\"\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "            predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "        print(f\"输入文本：'{text}'\")\n",
        "        print(f\"预测类别：{predictions.item()}\") # 具体类别标签需要根据模型训练时的设置进行映射\n",
        "        ```\n",
        "\n",
        "    *   **文本生成（使用仅解码器模型）**：\n",
        "        ```python\n",
        "        from transformers import set_seed\n",
        "        set_seed(42)\n",
        "\n",
        "        prompt_text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "        inputs = tokenizer(prompt_text, return_tensors=\"pt\")\n",
        "\n",
        "        # 生成文本，max_new_tokens控制生成长度\n",
        "        outputs = model.generate(inputs[\"input_ids\"], max_new_tokens=50, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        print(f\"起始文本：'{prompt_text}'\")\n",
        "        print(f\"生成文本：'{generated_text}'\")\n",
        "        ```\n",
        "\n",
        "**总结**：Hugging Face Transformers库极大地简化了SLM的部署和使用。通过简单的几行代码，就可以加载预训练的分词器和模型，并执行各种NLP任务。这使得开发者能够专注于应用逻辑，而不是底层的模型实现细节，从而高效地利用这些轻量级模型。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "304bda2e"
      },
      "source": [
        "## 训练环境设置\n",
        "\n",
        "### Subtask:\n",
        "提供关于设置所需软件（如PyTorch, TensorFlow, Hugging Face Transformers库）和硬件考虑的建议。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40229a68"
      },
      "source": [
        "### 深度学习框架和Hugging Face Transformers库介绍\n",
        "\n",
        "在自然语言处理（NLP）领域，特别是大型语言模型（SLM）的开发中，选择合适的深度学习框架和库至关重要。目前主流的深度学习框架包括 **PyTorch** 和 **TensorFlow**，它们都提供了强大的功能来构建和训练复杂的神经网络模型。\n",
        "\n",
        "*   **PyTorch**: 以其动态计算图（define-by-run）和Pythonic接口而闻名，深受研究人员喜爱，特别是在快速原型开发和实验方面。它提供了灵活的模型定义、易于调试的特性以及强大的GPU加速支持。\n",
        "\n",
        "*   **TensorFlow**: 由Google开发，以其静态计算图（define-and-run）和在生产环境中的可扩展性而著称。它提供了一整套工具和生态系统，包括TensorFlow Extended (TFX) 用于端到端机器学习平台，以及TensorFlow Lite和TensorFlow.js用于部署到边缘设备和浏览器。\n",
        "\n",
        "*   **Hugging Face Transformers**: 这是一个广受欢迎的库，它基于PyTorch和TensorFlow构建，提供了数千个预训练模型（如BERT, GPT-2, T5, Llama等）和用于NLP任务（如文本分类、问答、摘要、翻译等）的易用API。它极大地简化了SLM的开发和应用，使得开发者能够快速地加载、微调和部署最先进的模型。对于SLM项目，Hugging Face Transformers库几乎是不可或缺的组件，因为它提供了从模型架构到预训练权重再到高效的标记器（tokenizer）的一切所需。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df4e6044"
      },
      "source": [
        "### 软件安装指南\n",
        "\n",
        "#### Hugging Face Transformers\n",
        "\n",
        "您可以使用 `pip` 命令来安装 Hugging Face Transformers 库。请注意，它会自动安装其依赖项，包括 `torch` (PyTorch) 或 `tensorflow` (TensorFlow)，具体取决于您的环境或您是否明确指定。\n",
        "\n",
        "```python\n",
        "!pip install transformers\n",
        "```\n",
        "\n",
        "如果您还需要安装其他功能（例如，模型训练工具 `accelerate`），可以这样安装：\n",
        "\n",
        "```python\n",
        "!pip install transformers accelerate\n",
        "```\n",
        "\n",
        "#### PyTorch / TensorFlow (可选)\n",
        "\n",
        "虽然Hugging Face Transformers在安装时通常会拉取其兼容的PyTorch或TensorFlow版本，但如果您需要更精确地控制或安装特定版本，或者计划进行更底层的开发，建议您参考它们的官方安装指南：\n",
        "\n",
        "*   **PyTorch 官方安装指南**: [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)\n",
        "*   **TensorFlow 官方安装指南**: [https://www.tensorflow.org/install](https://www.tensorflow.org/install)\n",
        "\n",
        "这些指南将根据您的操作系统、Python版本和GPU支持需求提供详细的安装步骤。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate\n",
        "!pip install torch torchvision"
      ],
      "metadata": {
        "id": "4_JuAfw1g65v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "  print(f\"OK,device is {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "  print(\"NO\")"
      ],
      "metadata": {
        "id": "oDSFg18dixfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "650196c2"
      },
      "source": [
        "### 硬件环境考虑\n",
        "\n",
        "对于大型语言模型（SLM）的训练和推理，硬件选择对性能和效率至关重要。主要考虑因素是中央处理器（CPU）和图形处理器（GPU）的选择。\n",
        "\n",
        "*   **CPU**: CPU通常用于处理通用计算任务，对于模型的加载、数据预处理、批处理较小的模型推理或没有GPU支持时的训练是必需的。然而，对于大型SLM的训练和高性能推理，CPU的并行处理能力远不及GPU，会导致处理速度非常慢。\n",
        "\n",
        "*   **GPU**: GPU是深度学习，尤其是SLM训练和推理的核心。它们拥有大量的并行处理单元，可以高效地执行矩阵乘法和卷积等计算，这正是神经网络模型的核心操作。对于大型模型，GPU内存（VRAM）的大小至关重要，因为整个模型参数和中间激活都需要加载到其中。推荐使用NVIDIA的GPU，因为它们有强大的CUDA生态系统支持，这是绝大多数深度学习框架和库（如PyTorch, TensorFlow, Hugging Face Transformers）优化的基础。\n",
        "\n",
        "#### GPU在SLM训练和推理中的重要性：\n",
        "\n",
        "*   **训练**: 训练大型SLM需要极高的计算资源。一块高性能的GPU（例如NVIDIA A100, H100）可以显著缩短训练时间，并允许使用更大的批次大小和模型尺寸。对于更大的模型，可能需要多块GPU进行分布式训练。\n",
        "\n",
        "*   **推理**: 即使是推理，如果涉及到大型模型，GPU也能提供显著的速度优势。快速推理对于实时应用或处理大量请求至关重要。例如，一个简单的文本生成任务在CPU上可能需要几秒钟，但在GPU上可能只需要几十毫秒。GPU的显存大小决定了可以加载的模型尺寸。\n",
        "\n",
        "**总结**: 除非是小规模实验或资源有限，否则强烈建议使用至少一块高性能的GPU进行SLM的开发工作。对于严肃的训练任务，多块GPU配置或专业的AI加速器是必需的。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcc741eb"
      },
      "source": [
        "### Colab环境中GPU检查与启用\n",
        "\n",
        "Google Colaboratory（Colab）提供免费的GPU资源，这对于进行深度学习实验非常有帮助。您需要确保当前会话已启用GPU运行时。\n",
        "\n",
        "#### 检查GPU可用性\n",
        "\n",
        "您可以使用以下Python代码来检查当前Colab环境中是否提供了GPU：\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU is available! Device name:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"GPU is not available. Running on CPU.\")\n",
        "```\n",
        "\n",
        "如果您使用TensorFlow，可以这样检查：\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "\n",
        "if tf.test.is_gpu_available():\n",
        "    print(\"GPU is available! List of GPUs:\")\n",
        "    print(tf.config.list_physical_devices('GPU'))\n",
        "else:\n",
        "    print(\"GPU is not available. Running on CPU.\")\n",
        "```\n",
        "\n",
        "#### 启用GPU运行时\n",
        "\n",
        "如果上述代码显示GPU不可用，您需要手动在Colab中启用GPU运行时。请按照以下步骤操作：\n",
        "\n",
        "1.  点击Colab菜单栏的 `运行时 (Runtime)`。\n",
        "2.  选择 `更改运行时类型 (Change runtime type)`。\n",
        "3.  在“硬件加速器 (Hardware accelerator)”下拉菜单中，选择 `GPU`。\n",
        "4.  点击 `保存 (Save)`。\n",
        "\n",
        "更改运行时类型后，Colab可能会重启运行时。请重新运行GPU检查代码以确认GPU已成功启用。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f43be990"
      },
      "source": [
        "## 模型训练过程\n",
        "\n",
        "### Subtask:\n",
        "详细说明训练SLM的步骤，包括超参数调整、损失函数选择和优化策略。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e06576b"
      },
      "source": [
        "### 1. 数据准备（训练阶段）\n",
        "\n",
        "在SLM训练之前，需要将预处理和分词后的文本数据转换为模型可以理解的格式。通常，这涉及到将原始文本转换为token ID序列，并进行填充（padding）以确保序列长度一致，以及创建注意力掩码（attention mask）。\n",
        "\n",
        "- **PyTorch `Dataset`**: 这是加载和处理数据的抽象基类。你需要创建一个自定义的`Dataset`类，继承自`torch.utils.data.Dataset`，并实现`__len__`和`__getitem__`方法。\n",
        "  - `__len__` 方法返回数据集中样本的总数。\n",
        "  - `__getitem__` 方法根据索引返回单个样本，该样本通常是一个字典，包含token ID、注意力掩码和标签（如果适用）。\n",
        "\n",
        "- **PyTorch `DataLoader`**: `DataLoader`包装了`Dataset`，并提供了迭代数据批次（batches）的能力。它的主要作用包括：\n",
        "  - **批处理（Batching）**: 将多个独立样本组合成一个批次。批处理的重要性在于：\n",
        "    - **提高训练效率**: GPU在处理批量数据时效率更高，因为可以并行计算。\n",
        "    - **梯度稳定性**: 使用批次计算梯度可以减少梯度的方差，使训练过程更稳定。\n",
        "    - **内存管理**: 限制批次大小有助于控制模型训练时的内存消耗。\n",
        "  - **洗牌（Shuffling）**: 在每个epoch开始时随机打乱数据，防止模型学习到数据的顺序性而不是特征。\n",
        "  - **多进程数据加载**: 可以利用多个CPU核心异步加载数据，减少数据加载的瓶颈。\n",
        "\n",
        "**示例数据结构**：\n",
        "每个样本通常包含以下键值对：\n",
        "- `input_ids`: 经过分词器处理后的token ID序列。\n",
        "- `attention_mask`: 用于区分实际token和填充token的掩码。\n",
        "- `labels` (可选): 如果是监督学习任务，如文本分类或下一个词预测，则包含相应的标签或目标token ID。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfb7b40e"
      },
      "source": [
        "### 2. 超参数选择\n",
        "\n",
        "超参数是在模型训练之前设定的参数，它们对模型的性能和训练过程有显著影响。在训练SLM时，以下是一些重要的超参数及其选择建议：\n",
        "\n",
        "-   **学习率（Learning Rate）**:\n",
        "    -   **定义**: 学习率决定了模型在每次参数更新时，沿梯度方向前进的步长大小。一个合适的学习率可以帮助模型快速收敛到最优解，而过大或过小都可能导致问题。\n",
        "    -   **影响**:\n",
        "        -   **学习率过高**: 模型可能跳过最优解，导致训练不稳定甚至发散。\n",
        "        -   **学习率过低**: 模型收敛速度会非常慢，训练时间过长，并且可能陷入局部最优。\n",
        "    -   **选择建议**: 通常从一个较小的值开始尝试（例如，1e-5, 2e-5, 5e-5），并结合学习率调度器动态调整。可以进行网格搜索或随机搜索，或者使用学习率查找器（如`lr_finder`）来找到一个合适的初始值。\n",
        "\n",
        "-   **批次大小（Batch Size）**:\n",
        "    -   **定义**: 批次大小是指在每次模型参数更新前，用于计算梯度的数据样本数量。\n",
        "    -   **影响**:\n",
        "        -   **批次大小过大**: 需要更多的内存，并且一个epoch内的更新次数减少，可能导致泛化能力下降（通常称为“大批次泛化差距”）。但计算效率可能更高，因为GPU并行度高。\n",
        "        -   **批次大小过小**: 梯度估计的噪声会增大，训练可能不稳定，但有助于提高泛化能力并避免局部最优。一个epoch内的更新次数增多，但总训练时间可能增加。\n",
        "    -   **选择建议**: 批次大小通常受限于GPU内存。常见的批次大小有8、16、32、64等。通常从小批次开始，并逐渐增大，观察验证集性能，选择在内存允许范围内且性能最佳的批次大小。\n",
        "\n",
        "-   **训练轮次（Epochs）**:\n",
        "    -   **定义**: 一个epoch表示模型已经遍历了整个训练数据集一次。\n",
        "    -   **影响**:\n",
        "        -   **Epochs过少**: 模型可能欠拟合，未能充分学习数据中的模式。\n",
        "        -   **Epochs过多**: 模型可能过拟合训练数据，导致在未见过的数据上性能下降。\n",
        "    -   **选择建议**: 采用早停（early stopping）策略是一个好方法。即监控验证集上的性能，当验证集性能在一定数量的epoch内不再提升时，停止训练。\n",
        "\n",
        "-   **序列长度（Sequence Length）**:\n",
        "    -   **定义**: 输入模型文本序列的最大token数量。\n",
        "    -   **影响**:\n",
        "        -   **序列长度过长**: 增加计算复杂度和内存消耗，尤其是对于Transformer模型，其注意力机制的计算复杂度与序列长度的平方成正比。可能捕获更长的上下文信息。\n",
        "        -   **序列长度过短**: 可能丢失重要的上下文信息，导致模型性能下降。\n",
        "    -   **选择建议**: 根据任务需求和计算资源进行选择。对于大多数任务，可以分析训练数据中序列长度的分布，选择一个覆盖大部分序列的长度，或根据模型的预训练长度来设定。常用的序列长度有128、256、512等。\n",
        "\n",
        "-   **权重衰减（Weight Decay）/L2正则化**:\n",
        "    -   **定义**: 一种正则化技术，通过在损失函数中添加与模型权重平方成正比的项来惩罚大的权重，从而防止过拟合。\n",
        "    -   **影响**: 可以有效防止模型过拟合训练数据，提高泛化能力。\n",
        "    -   **选择建议**: 通常在优化器中设定，例如AdamW优化器中直接包含权重衰减参数。常见的值有0.01。可以通过实验找到最佳值。\n",
        "\n",
        "-   **学习率调度器（Learning Rate Scheduler）**:\n",
        "    -   **定义**: 在训练过程中动态调整学习率的策略，例如线性衰减、余弦退火、warmup等。\n",
        "    -   **影响**: 合理的学习率调度可以帮助模型在训练初期快速学习，在后期更稳定地收敛，从而提高训练效率和模型性能。\n",
        "    -   **选择建议**: 常见的策略包括：先进行warmup（学习率从0逐渐增大），然后线性衰减到0，或采用余弦退火策略。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caca3fc2"
      },
      "source": [
        "### 3. 损失函数\n",
        "\n",
        "损失函数（Loss Function）用于衡量模型预测值与真实值之间的差异。在SLM训练中，目标通常是预测序列中的下一个词或下一个token，因此这类任务通常被视为一个多分类问题，最常用的损失函数是交叉熵损失（Cross-Entropy Loss）。\n",
        "\n",
        "-   **交叉熵损失（Cross-Entropy Loss）**:\n",
        "    -   **定义**: 交叉熵损失衡量的是两个概率分布之间的差异。在分类任务中，它衡量模型输出的概率分布与真实标签的one-hot编码分布之间的差异。对于语言模型，它计算的是模型预测的下一个token的概率分布与实际下一个token的分布之间的交叉熵。\n",
        "    -   **公式**: 对于一个给定的token，如果真实标签是 $y_i$（在one-hot编码中为1，其他为0），模型预测的对应token的概率是 $p_i$，则单样本的交叉熵损失为：\n",
        "        $$L = -\\sum_{i} y_i \\log(p_i)$$\n",
        "        由于真实标签只有一个位置是1，其余是0，这个公式可以简化为：\n",
        "        $$L = -\\log(p_{\\text{true_token}})$$\n",
        "        即真实token被模型预测的概率的负对数。在批量处理时，会计算批次中所有token的平均损失。\n",
        "    -   **作用**:\n",
        "        -   **度量预测准确性**: 交叉熵损失函数能够很好地反映模型预测的token与真实token之间的差距。当模型预测的概率分布与真实分布越接近时，损失值越小。\n",
        "        -   **鼓励置信度**: 它不仅惩罚模型预测错误，而且还惩罚模型对其正确预测缺乏置信度的情况。例如，如果模型正确预测了下一个token，但给出的概率很低，损失仍然会相对较高。\n",
        "        -   **适用于分类任务**: 语言模型的训练本质上是一个多分类任务，即在词汇表中的所有可能token中选择下一个最可能的token，因此交叉熵损失非常适合这类任务。\n",
        "\n",
        "在PyTorch等深度学习框架中，通常会使用`torch.nn.CrossEntropyLoss`，它会自动执行Softmax操作（将模型的原始输出logits转换为概率分布）然后计算交叉熵。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b0b1d10"
      },
      "source": [
        "### 4. 优化器与学习率调度器\n",
        "\n",
        "优化器（Optimizer）和学习率调度器（Learning Rate Scheduler）是训练过程中至关重要的组件，它们协同工作以有效地更新模型参数，从而使模型收敛并达到更好的性能。\n",
        "\n",
        "-   **优化器（Optimizer）**:\n",
        "    -   **定义**: 优化器是一种算法，用于根据损失函数计算出的梯度来更新模型的权重和偏置，以最小化损失函数。\n",
        "    -   **AdamW**: `AdamW`是`Adam`优化器的一种变体，它将权重衰减（Weight Decay）从`Adam`的L2正则化中解耦出来，使得权重衰减能够更有效地应用于模型参数。在实践中，`AdamW`在许多深度学习任务，特别是Transformer模型中表现出色，因为它能更好地防止过拟合。\n",
        "    -   **其他常见优化器**:\n",
        "        -   **SGD（Stochastic Gradient Descent）**: 最基本的优化器，每次使用一个批次的数据更新参数。虽然简单，但结合合适的学习率调度，有时也能达到很好的效果。\n",
        "        -   **Adam（Adaptive Moment Estimation）**: 一种自适应学习率优化算法，结合了Momentum和RMSProp的优点，为每个参数计算不同的自适应学习率。\n",
        "    -   **选择建议**: 对于SLM，`AdamW`通常是首选的优化器，因为它在处理大规模模型和防止过拟合方面表现良好。\n",
        "\n",
        "-   **学习率调度器（Learning Rate Scheduler）**:\n",
        "    -   **定义**: 学习率调度器是一种策略，用于在训练过程中动态调整学习率。在训练的不同阶段使用不同的学习率有助于模型更好地收敛。\n",
        "    -   **Warmup**:\n",
        "        -   **定义**: 在训练初期，学习率从一个非常小的值（通常是0）逐渐线性增加到一个预设的初始学习率。这被称为“热身”阶段。\n",
        "        -   **作用**: 在训练初期，模型参数是随机初始化的，大的学习率可能导致模型不稳定。Warmup可以帮助模型在训练初期稳定下来，避免大的梯度更新造成的震荡，从而有助于更好地探索损失函数的曲面。\n",
        "    -   **线性衰减（Linear Decay）**:\n",
        "        -   **定义**: 在Warmup阶段结束后，学习率从初始学习率线性地逐渐降低到0。\n",
        "        -   **作用**: 随着训练的进行，模型逐渐接近最优解，此时需要较小的学习率进行微调，以避免在最优解附近来回震荡，实现更精确的收敛。\n",
        "    -   **余弦退火（Cosine Annealing）**:\n",
        "        -   **定义**: 学习率按照余弦函数的形式周期性地下降和上升。通常，学习率从初始值开始下降，在周期的中间达到最小值，然后再次上升。\n",
        "        -   **作用**: 这种调度器可以帮助模型跳出局部最优，并探索损失函数的不同区域，从而找到更好的全局最优解。\n",
        "    -   **选择建议**: 常见的策略是结合Warmup和线性衰减或余弦退火。例如，先进行几个epoch的Warmup，然后学习率线性衰减到0。这种组合在许多Transformer模型训练中被广泛使用并证明有效。\n",
        "\n",
        "**如何影响收敛和性能**:\n",
        "-   **优化器**选择不当可能导致模型收敛速度慢、无法收敛或陷入次优解。\n",
        "-   **学习率调度器**可以帮助模型在训练初期快速学习，并在后期进行精细调整，从而提高收敛速度和最终的模型性能，并有效避免过拟合。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33b34fdd"
      },
      "source": [
        "### 5. 训练循环概述\n",
        "\n",
        "典型的SLM训练循环涉及到重复以下步骤，通常在一个或多个epoch中进行，每个epoch遍历一次整个训练数据集：\n",
        "\n",
        "1.  **数据加载**：从 `DataLoader` 中获取一个批次的输入数据（`input_ids`、`attention_mask`、`labels`等）。\n",
        "\n",
        "2.  **前向传播（Forward Pass）**：\n",
        "    -   将批次数据输入到模型中。\n",
        "    -   模型计算输出，例如预测的下一个token的logit。\n",
        "\n",
        "3.  **计算损失（Calculate Loss）**：\n",
        "    -   使用模型的输出（logit）和真实标签（`labels`）计算损失值（例如，使用交叉熵损失）。\n",
        "\n",
        "4.  **反向传播（Backward Pass）**：\n",
        "    -   计算损失函数相对于模型所有可训练参数的梯度（`loss.backward()`）。\n",
        "\n",
        "5.  **参数更新（Parameter Update）**：\n",
        "    -   使用优化器（例如`AdamW`）和计算出的梯度来更新模型的参数（`optimizer.step()`）。\n",
        "    -   **清零梯度**：在每次参数更新后，清除之前计算的梯度，以准备下一次迭代（`optimizer.zero_grad()`）。\n",
        "\n",
        "6.  **学习率调度**：根据学习率调度器（如果使用）更新学习率。\n",
        "\n",
        "7.  **日志记录与评估**：\n",
        "    -   定期记录损失、准确率等指标，以便监控训练进度。\n",
        "    -   在验证集上评估模型性能，以检查过拟合和选择最佳模型。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "399612d7"
      },
      "source": [
        "### 6. 使用Hugging Face Trainer API\n",
        "\n",
        "对于训练SLM，尤其是在处理大型语言模型和复杂的训练配置时，Hugging Face 的 `Trainer` API 提供了一个高度抽象和功能丰富的接口，极大地简化了训练过程。\n",
        "\n",
        "**`Trainer` API 的主要优势**：\n",
        "\n",
        "-   **简化训练循环**：`Trainer` 封装了标准的训练循环，包括前向传播、损失计算、反向传播、参数更新、学习率调度、梯度清零等所有必要步骤。\n",
        "-   **内置功能**：\n",
        "    -   **分布式训练**：原生支持多GPU、多节点分布式训练，无需手动管理DDP（Distributed Data Parallel）。\n",
        "    -   **混合精度训练**：自动启用FP16/BF16混合精度训练，减少内存占用并加速训练，同时保持模型性能。\n",
        "    -   **梯度累积**：支持梯度累积，允许使用较小的批次大小模拟更大的批次效果，有助于解决显存限制问题。\n",
        "    -   **日志记录与评估**：集成了日志记录（如TensorBoard、Weights & Biases）和定期在验证集上评估模型性能的功能。\n",
        "    -   **检查点与恢复**：自动保存和加载模型检查点，方便中断后继续训练或部署最佳模型。\n",
        "    -   **超参数搜索**：可以与超参数优化库（如Optuna、Ray Tune）集成，进行超参数调优。\n",
        "    -   **自定义回调（Callbacks）**：允许用户定义自定义行为，例如在训练过程中执行特定操作，如早停（Early Stopping）。\n",
        "-   **与`transformers`库集成**：`Trainer`与Hugging Face `transformers`库中的模型、分词器和数据集高度集成，提供了无缝的用户体验。\n",
        "\n",
        "**使用示例（概念性）**：\n",
        "\n",
        "通常，使用`Trainer`进行训练的步骤如下：\n",
        "\n",
        "1.  **定义模型**：加载一个预训练的SLM模型，或者定义一个自定义模型。\n",
        "2.  **定义分词器**：加载与模型对应的分词器。\n",
        "3.  **准备数据**：创建 `torch.utils.data.Dataset` 对象（通常通过 `datasets` 库或自定义方式）。\n",
        "4.  **配置训练参数**：使用 `TrainingArguments` 类来定义所有训练相关的超参数和配置（例如学习率、批次大小、epochs、权重衰减、输出目录、日志步长等）。\n",
        "5.  **实例化 `Trainer`**：将模型、训练参数、训练数据集、验证数据集、优化器（可选）、学习率调度器（可选）和数据整理器（Data Collator）传入 `Trainer`。\n",
        "6.  **调用 `train()` 方法**：通过 `trainer.train()` 启动训练。\n",
        "\n",
        "`Trainer` API 的使用极大地减少了编写和管理训练代码的复杂性，使得开发者可以更专注于模型架构和数据本身。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8be9c68a"
      },
      "source": [
        "### 7. 模型保存与加载\n",
        "\n",
        "在模型训练过程中，定期保存模型的权重和优化器状态是至关重要的，这可以确保即使训练中断也能从上次保存点恢复，或者在训练结束后部署最佳模型进行推理。此外，加载已训练好的模型进行微调或迁移学习也是常见的做法。\n",
        "\n",
        "-   **模型保存**:\n",
        "    -   **目的**：保存模型的状态，以便在未来使用（例如，进行推理、继续训练或分享模型）。\n",
        "    -   **保存内容**：通常需要保存以下两部分：\n",
        "        1.  **模型权重（Model Weights/State Dict）**：这是模型学到的参数。在PyTorch中，通常通过`model.state_dict()`获取，然后使用`torch.save()`保存为一个`.pt`或`.pth`文件。例如：\n",
        "            ```python\n",
        "            torch.save(model.state_dict(), 'model_weights.pth')\n",
        "            ```\n",
        "        2.  **优化器状态（Optimizer State Dict）** (如果需要继续训练)：保存优化器的内部状态（例如Adam中的动量项），这样可以从中断处恢复训练，而不仅仅是从头开始。例如：\n",
        "            ```python\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': loss,\n",
        "                # ... 其他需要保存的信息\n",
        "            }, 'checkpoint_epoch_X.pt')\n",
        "            ```\n",
        "    -   **最佳实践**：\n",
        "        -   **定期保存检查点**：在每个epoch结束或每隔N个步数保存一次模型。\n",
        "        -   **保存最佳模型**：根据验证集上的性能指标（如准确率、F1分数等）保存表现最好的模型。\n",
        "        -   **明确文件名**：使用有意义的文件名，包含epoch、损失值或评估指标等信息。\n",
        "\n",
        "-   **模型加载**:\n",
        "    -   **目的**：将之前保存的模型状态恢复到模型和优化器中。\n",
        "    -   **加载模型权重进行推理**：\n",
        "        1.  首先，需要创建与保存时相同的模型架构。\n",
        "        2.  然后，使用`model.load_state_dict()`加载保存的权重。例如：\n",
        "            ```python\n",
        "            model = YourSLMModel(...) # 重新创建模型实例\n",
        "            model.load_state_dict(torch.load('model_weights.pth'))\n",
        "            model.eval() # 设置为评估模式\n",
        "            ```\n",
        "    -   **加载检查点继续训练**：\n",
        "        1.  创建模型和优化器实例。\n",
        "        2.  加载完整的检查点字典，并分别加载模型和优化器状态。例如：\n",
        "            ```python\n",
        "            model = YourSLMModel(...) # 重新创建模型实例\n",
        "            optimizer = torch.optim.AdamW(model.parameters(), lr=...) # 重新创建优化器实例\n",
        "\n",
        "            checkpoint = torch.load('checkpoint_epoch_X.pt')\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            epoch = checkpoint['epoch']\n",
        "            loss = checkpoint['loss']\n",
        "            \n",
        "            model.train() # 设置为训练模式\n",
        "            # 从'epoch'继续训练\n",
        "            ```\n",
        "    -   **Hugging Face `Trainer` 的简化**：如果使用Hugging Face `Trainer`，它会自动处理检查点保存和加载。你可以通过`TrainingArguments`中的`output_dir`和`save_strategy`参数控制保存行为，并通过`Trainer.train(resume_from_checkpoint=True)`来恢复训练。\n",
        "\n",
        "妥善地管理模型保存和加载是确保深度学习项目可复现性和高效性的关键。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85f9f337"
      },
      "source": [
        "### 7. 模型保存与加载\n",
        "\n",
        "在模型训练过程中，定期保存模型的权重和优化器状态是至关重要的，这可以确保即使训练中断也能从上次保存点恢复，或者在训练结束后部署最佳模型进行推理。此外，加载已训练好的模型进行微调或迁移学习也是常见的做法。\n",
        "\n",
        "-   **模型保存**:\n",
        "    -   **目的**：保存模型的状态，以便在未来使用（例如，进行推理、继续训练或分享模型）。\n",
        "    -   **保存内容**：通常需要保存以下两部分：\n",
        "        1.  **模型权重（Model Weights/State Dict）**：这是模型学到的参数。在PyTorch中，通常通过`model.state_dict()`获取，然后使用`torch.save()`保存为一个`.pt`或`.pth`文件。例如：\n",
        "            ```python\n",
        "            torch.save(model.state_dict(), 'model_weights.pth')\n",
        "            ```\n",
        "        2.  **优化器状态（Optimizer State Dict）** (如果需要继续训练)：保存优化器的内部状态（例如Adam中的动量项），这样可以从中断处恢复训练，而不仅仅是从头开始。例如：\n",
        "            ```python\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': loss,\n",
        "                # ... 其他需要保存的信息\n",
        "            }, 'checkpoint_epoch_X.pt')\n",
        "            ```\n",
        "    -   **最佳实践**：\n",
        "        -   **定期保存检查点**：在每个epoch结束或每隔N个步数保存一次模型。\n",
        "        -   **保存最佳模型**：根据验证集上的性能指标（如准确率、F1分数等）保存表现最好的模型。\n",
        "        -   **明确文件名**：使用有意义的文件名，包含epoch、损失值或评估指标等信息。\n",
        "\n",
        "-   **模型加载**:\n",
        "    -   **目的**：将之前保存的模型状态恢复到模型和优化器中。\n",
        "    -   **加载模型权重进行推理**：\n",
        "        1.  首先，需要创建与保存时相同的模型架构。\n",
        "        2.  然后，使用`model.load_state_dict()`加载保存的权重。例如：\n",
        "            ```python\n",
        "            model = YourSLMModel(...) # 重新创建模型实例\n",
        "            model.load_state_dict(torch.load('model_weights.pth'))\n",
        "            model.eval() # 设置为评估模式\n",
        "            ```\n",
        "    -   **加载检查点继续训练**：\n",
        "        1.  创建模型和优化器实例。\n",
        "        2.  加载完整的检查点字典，并分别加载模型和优化器状态。例如：\n",
        "            ```python\n",
        "            model = YourSLMModel(...) # 重新创建模型实例\n",
        "            optimizer = torch.optim.AdamW(model.parameters(), lr=...) # 重新创建优化器实例\n",
        "\n",
        "            checkpoint = torch.load('checkpoint_epoch_X.pt')\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            epoch = checkpoint['epoch']\n",
        "            loss = checkpoint['loss']\n",
        "            \n",
        "            model.train() # 设置为训练模式\n",
        "            # 从'epoch'继续训练\n",
        "            ```\n",
        "    -   **Hugging Face `Trainer` 的简化**：如果使用Hugging Face `Trainer`，它会自动处理检查点保存和加载。你可以通过`TrainingArguments`中的`output_dir`和`save_strategy`参数控制保存行为，并通过`Trainer.train(resume_from_checkpoint=True)`来恢复训练。\n",
        "\n",
        "妥善地管理模型保存和加载是确保深度学习项目可复现性和高效性的关键。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e820ab9d"
      },
      "source": [
        "## 模型评估\n",
        "\n",
        "### Subtask:\n",
        "描述如何使用适当的指标和基准来评估训练好的SLM的性能。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f3a3a86"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **SLM Fundamentals:** Small Language Models (SLMs) offer advantages over Large Language Models (LLMs) in terms of **efficiency, deployment flexibility, and cost-effectiveness** due to their smaller parameter count and reduced resource demands. Their core components, such as the Transformer architecture with self-attention, enable efficient sequence processing and understanding.\n",
        "*   **SLM Development Lifecycle:** A comprehensive eight-stage iterative lifecycle was outlined, starting from data collection and preprocessing, through model selection, training, evaluation, and optimization, to deployment, monitoring, and maintenance.\n",
        "*   **Data Preparation:**\n",
        "    *   **Data Sources:** Emphasized the use of both generic (e.g., Common Crawl, Wikipedia) and domain-specific datasets, leveraging platforms like Hugging Face Datasets and Kaggle, or custom web scraping.\n",
        "    *   **Preprocessing Steps:** Key cleaning steps include removing HTML tags, special characters, converting to lowercase, and eliminating duplicate content, with practical Python examples provided for each.\n",
        "    *   **Tokenization:** Highlighted the importance of subword tokenization (e.g., BPE, WordPiece) using tools like Hugging Face's `AutoTokenizer` to handle out-of-vocabulary words and convert text into numerical `input_ids` and `attention_mask` for model input.\n",
        "*   **Model Architecture Selection:**\n",
        "    *   **Core Architecture:** Transformer models, particularly encoder-only (e.g., DistilBERT, TinyBERT, MobileBERT) for understanding tasks and decoder-only (e.g., GPT-2 small, GPT-Neo) for generation, are the recommended base for SLMs.\n",
        "    *   **Compression Techniques:** Model compression methods like **Knowledge Distillation** (e.g., DistilBERT reduces parameters by 40% and speeds up by 60% with less than 3% performance drop), **Model Pruning**, and **Quantization** are crucial for adapting larger models to resource-constrained SLM environments.\n",
        "*   **Training Environment Setup:**\n",
        "    *   **Software:** PyTorch and TensorFlow were identified as primary deep learning frameworks, with the Hugging Face Transformers library being indispensable for SLM development.\n",
        "    *   **Hardware:** GPUs are critical for efficient SLM training and inference due to their parallel processing capabilities and memory (VRAM) requirements; NVIDIA GPUs with CUDA support are highly recommended. Practical steps for enabling and verifying GPU usage in Google Colab were provided.\n",
        "*   **Model Training Process:**\n",
        "    *   **Data Handling:** `torch.utils.data.Dataset` and `DataLoader` are essential for batching and shuffling preprocessed data, which improves training efficiency and gradient stability.\n",
        "    *   **Hyperparameters:** Guidance was given on critical hyperparameters:\n",
        "        *   **Learning Rate:** Typically starts small (e.g., 1e-5 to 5e-5) with dynamic adjustment through schedulers.\n",
        "        *   **Batch Size:** Varies (e.g., 8, 16, 32, 64) constrained by GPU memory.\n",
        "        *   **Epochs:** Managed with early stopping to prevent overfitting.\n",
        "        *   **Sequence Length:** Chosen based on task and resources (e.g., 128, 256, 512).\n",
        "        *   **Weight Decay:** Commonly set around 0.01 for regularization.\n",
        "    *   **Loss Function:** Cross-Entropy Loss is the standard for SLM training, treating next-token prediction as a multi-classification problem.\n",
        "    *   **Optimizer & Scheduler:** `AdamW` is the preferred optimizer for SLMs, often combined with learning rate schedulers like Warmup and Linear Decay for stable and efficient convergence.\n",
        "    *   **Training Loop & Automation:** A typical training loop was outlined, and the Hugging Face `Trainer` API was highlighted for its ability to abstract away complex training mechanics, including distributed training, mixed precision, and checkpoint management.\n",
        "    *   **Model Persistence:** Emphasized the importance of saving and loading model weights and optimizer states for continuity and deployment, with methods provided for both manual and `Trainer`-assisted saving/loading.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Leverage Hugging Face's ecosystem (Transformers, Datasets, `Trainer` API) as the primary toolkit for SLM development, as it streamlines complex processes from data preparation to training and deployment.\n",
        "*   Prioritize model compression techniques (distillation, pruning, quantization) early in the SLM development cycle to achieve optimal balance between performance and resource efficiency, especially for target deployment environments like edge devices.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d5b2423"
      },
      "source": [
        "# Task\n",
        "Develop a small language model (SLM) for text classification. This will involve: setting up the environment, preparing a text classification dataset using Hugging Face `datasets` and `AutoTokenizer`, selecting and loading a suitable SLM (e.g., `distilbert-base-uncased`), defining evaluation metrics, configuring training parameters with `TrainingArguments`, initializing and training the model using `Trainer`, evaluating the trained model on a test set, saving and reloading the model, and finally, demonstrating inference with new text samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2137616f"
      },
      "source": [
        "## 环境设置与库安装\n",
        "\n",
        "### Subtask:\n",
        "确保所有必要的库（如`transformers`, `accelerate`, `torch`, `datasets`, `scikit-learn`）都已安装，并检查GPU是否可用。这一步将验证之前已执行的代码单元是否成功安装了这些库。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95d3652d"
      },
      "source": [
        "### 验证库安装和GPU可用性\n",
        "\n",
        "根据之前的执行结果，请检查以下信息以确认环境设置成功：\n",
        "\n",
        "1.  **库安装确认**：\n",
        "    *   请查看 `4_JuAfw1g65v` 代码单元的输出。您应该看到 `transformers` 和 `accelerate` 以及 `torch` 和 `torchvision` 显示为 `Requirement already satisfied` 或 `Successfully installed`。\n",
        "\n",
        "2.  **GPU可用性确认**：\n",
        "    *   请查看 `oDSFg18dixfq` 代码单元的输出。您应该看到类似于 `OK,device is Tesla T4` 的信息，表示GPU已被成功检测和识别。如果显示 `NO` 或 `GPU is not available`，请确保您已在Colab运行时设置中选择了GPU硬件加速器（`运行时` -> `更改运行时类型` -> `硬件加速器` 选择 `GPU`）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2a1eacd"
      },
      "source": [
        "## 数据准备\n",
        "\n",
        "### Subtask:\n",
        "我们将使用Hugging Face `datasets`库加载一个小型文本分类数据集（例如：`imdb`），对其进行预处理和分词。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08a5e262"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires loading a dataset, preprocessing, and tokenizing it. This code block will perform all these steps using Hugging Face libraries, as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5265db67"
      },
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# 1. Load the imdb dataset\n",
        "print(\"Loading imdb dataset...\")\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "train_dataset = dataset[\"train\"]\n",
        "test_dataset = dataset[\"test\"]\n",
        "\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "print(\"Example from train_dataset:\")\n",
        "print(train_dataset[0])\n",
        "\n",
        "# 2. Choose a tokenizer suitable for an SLM (e.g., distilbert-base-uncased)\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "print(f\"Tokenizer loaded: {model_name}\")\n",
        "\n",
        "# 3. Define a tokenize_function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=128)\n",
        "\n",
        "# 4. Apply the tokenize_function to the datasets\n",
        "print(\"Tokenizing train dataset...\")\n",
        "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "print(\"Tokenizing test dataset...\")\n",
        "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "print(\"Example from tokenized_train_dataset:\")\n",
        "print(tokenized_train_dataset[0])\n",
        "\n",
        "# 5. Set dataset format to PyTorch 'torch'\n",
        "print(\"Setting dataset format to torch...\")\n",
        "tokenized_train_dataset.set_format(\"torch\")\n",
        "tokenized_test_dataset.set_format(\"torch\")\n",
        "\n",
        "print(\"Data preparation and tokenization complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eac0bd2"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block successfully completed the data preparation and tokenization for the `imdb` dataset as per the subtask instructions. The `remove_columns=['text']` in the `.map()` function is correct, as the `label` column is necessary for the text classification task. I will reiterate the successful code to indicate this step is complete and correct.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2263a2ec"
      },
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# 1. Load the imdb dataset\n",
        "print(\"Loading imdb dataset...\")\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "train_dataset = dataset[\"train\"]\n",
        "test_dataset = dataset[\"test\"]\n",
        "\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "print(\"Example from train_dataset:\")\n",
        "print(train_dataset[0])\n",
        "\n",
        "# 2. Choose a tokenizer suitable for an SLM (e.g., distilbert-base-uncased)\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "print(f\"Tokenizer loaded: {model_name}\")\n",
        "\n",
        "# 3. Define a tokenize_function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, max_length=128)\n",
        "\n",
        "# 4. Apply the tokenize_function to the datasets\n",
        "print(\"Tokenizing train dataset...\")\n",
        "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "print(\"Tokenizing test dataset...\")\n",
        "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "print(\"Example from tokenized_train_dataset:\")\n",
        "print(tokenized_train_dataset[0])\n",
        "\n",
        "# 5. Set dataset format to PyTorch 'torch'\n",
        "print(\"Setting dataset format to torch...\")\n",
        "tokenized_train_dataset.set_format(\"torch\")\n",
        "tokenized_test_dataset.set_format(\"torch\")\n",
        "\n",
        "print(\"Data preparation and tokenization complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3050d65e"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step successfully prepared the data. Now, as per the subtask, I will load a suitable SLM for text classification, define the evaluation metrics, and set up the training arguments using Hugging Face's `TrainingArguments` and `AutoModelForSequenceClassification`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be36877e"
      },
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "# 1. Load a suitable SLM for text classification\n",
        "# We will use 'distilbert-base-uncased' as it is a small, efficient Transformer model.\n",
        "# The imdb dataset has 2 labels (positive/negative sentiment).\n",
        "num_labels = len(dataset[\"train\"].unique(\"label\"))\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "\n",
        "print(f\"Model loaded: {model_name} with {num_labels} labels.\")\n",
        "\n",
        "# 2. Define evaluation metrics\n",
        "# For text classification, accuracy, precision, recall, and f1 are standard metrics.\n",
        "metric = evaluate.load(\"glue\", \"mrpc\") # mrpc provides accuracy and f1, which are generally useful\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "print(\"Evaluation metrics defined (accuracy, f1).\")\n",
        "\n",
        "# 3. Configure training parameters with TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",             # output directory\n",
        "    num_train_epochs=3,                 # total number of training epochs\n",
        "    per_device_train_batch_size=16,     # batch size per device during training\n",
        "    per_device_eval_batch_size=16,      # batch size for evaluation\n",
        "    warmup_steps=500,                   # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,                  # strength of weight decay\n",
        "    logging_dir=\"./logs\",               # directory for storing logs\n",
        "    logging_steps=100,                  # log every x updates steps\n",
        "    evaluation_strategy=\"epoch\",        # evaluate each epoch\n",
        "    save_strategy=\"epoch\",              # save checkpoint each epoch\n",
        "    load_best_model_at_end=True,        # load the best model when training ends\n",
        "    metric_for_best_model=\"accuracy\",   # use accuracy to determine the best model\n",
        "    report_to=\"none\"                    # Disable integrations for simplicity\n",
        ")\n",
        "\n",
        "print(\"Training arguments configured.\")\n",
        "\n",
        "# Display some training arguments for verification\n",
        "print(f\"\\nTraining Arguments Summary:\")\n",
        "print(f\"  Number of epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"  Train batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"  Evaluation strategy: {training_args.evaluation_strategy}\")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8656e738"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block failed because the 'evaluate' library was not found. To fix this, I need to install the 'evaluate' library before importing it. I will add `!pip install evaluate` at the beginning of the code block to ensure it's available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fa7bd90f"
      },
      "source": [
        "!pip install evaluate\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "# 1. Load a suitable SLM for text classification\n",
        "# We will use 'distilbert-base-uncased' as it is a small, efficient Transformer model.\n",
        "# The imdb dataset has 2 labels (positive/negative sentiment).\n",
        "num_labels = len(dataset[\"train\"].unique(\"label\"))\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "\n",
        "print(f\"Model loaded: {model_name} with {num_labels} labels.\")\n",
        "\n",
        "# 2. Define evaluation metrics\n",
        "# For text classification, accuracy, precision, recall, and f1 are standard metrics.\n",
        "metric = evaluate.load(\"glue\", \"mrpc\") # mrpc provides accuracy and f1, which are generally useful\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "print(\"Evaluation metrics defined (accuracy, f1).\")\n",
        "\n",
        "# 3. Configure training parameters with TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",             # output directory\n",
        "    num_train_epochs=3,                 # total number of training epochs\n",
        "    per_device_train_batch_size=16,     # batch size per device during training\n",
        "    per_device_eval_batch_size=16,      # batch size for evaluation\n",
        "    warmup_steps=500,                   # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,                  # strength of weight decay\n",
        "    logging_dir=\"./logs\",               # directory for storing logs\n",
        "    logging_steps=100,                  # log every x updates steps\n",
        "    evaluation_strategy=\"epoch\",        # evaluate each epoch\n",
        "    save_strategy=\"epoch\",              # save checkpoint each epoch\n",
        "    load_best_model_at_end=True,        # load the best model when training ends\n",
        "    metric_for_best_model=\"accuracy\",   # use accuracy to determine the best model\n",
        "    report_to=\"none\"                    # Disable integrations for simplicity\n",
        ")\n",
        "\n",
        "print(\"Training arguments configured.\")\n",
        "\n",
        "# Display some training arguments for verification\n",
        "print(f\"\\nTraining Arguments Summary:\")\n",
        "print(f\"  Number of epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"  Train batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"  Evaluation strategy: {training_args.evaluation_strategy}\")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4b65b23"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block failed with a `TypeError` because the `evaluation_strategy` argument in `TrainingArguments` has been deprecated. It should be replaced with `eval_strategy`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9d6592ab"
      },
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "# 1. Load a suitable SLM for text classification\n",
        "# We will use 'distilbert-base-uncased' as it is a small, efficient Transformer model.\n",
        "# The imdb dataset has 2 labels (positive/negative sentiment).\n",
        "num_labels = len(dataset[\"train\"].unique(\"label\"))\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "\n",
        "print(f\"Model loaded: {model_name} with {num_labels} labels.\")\n",
        "\n",
        "# 2. Define evaluation metrics\n",
        "# For text classification, accuracy, precision, recall, and f1 are standard metrics.\n",
        "metric = evaluate.load(\"glue\", \"mrpc\") # mrpc provides accuracy and f1, which are generally useful\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "print(\"Evaluation metrics defined (accuracy, f1).\")\n",
        "\n",
        "# 3. Configure training parameters with TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",             # output directory\n",
        "    num_train_epochs=3,                 # total number of training epochs\n",
        "    per_device_train_batch_size=16,     # batch size per device during training\n",
        "    per_device_eval_batch_size=16,      # batch size for evaluation\n",
        "    warmup_steps=500,                   # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,                  # strength of weight decay\n",
        "    logging_dir=\"./logs\",               # directory for storing logs\n",
        "    logging_steps=100,                  # log every x updates steps\n",
        "    eval_strategy=\"epoch\",        # evaluate each epoch\n",
        "    save_strategy=\"epoch\",              # save checkpoint each epoch\n",
        "    load_best_model_at_end=True,        # load the best model when training ends\n",
        "    metric_for_best_model=\"accuracy\",   # use accuracy to determine the best model\n",
        "    report_to=\"none\"                    # Disable integrations for simplicity\n",
        ")\n",
        "\n",
        "print(\"Training arguments configured.\")\n",
        "\n",
        "# Display some training arguments for verification\n",
        "print(f\"\\nTraining Arguments Summary:\")\n",
        "print(f\"  Number of epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"  Train batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"  Evaluation strategy: {training_args.eval_strategy}\")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae136c13"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block failed with a `TypeError` because the `Trainer` no longer accepts `tokenizer` as a direct argument. The `tokenizer` should be passed to a `DataCollatorWithPadding` which is then used by the `Trainer`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d143c438"
      },
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "\n",
        "# 1. Load a suitable SLM for text classification\n",
        "# We will use 'distilbert-base-uncased' as it is a small, efficient Transformer model.\n",
        "# The imdb dataset has 2 labels (positive/negative sentiment).\n",
        "num_labels = len(dataset[\"train\"].unique(\"label\"))\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "\n",
        "print(f\"Model loaded: {model_name} with {num_labels} labels.\")\n",
        "\n",
        "# 2. Define evaluation metrics\n",
        "# For text classification, accuracy, precision, recall, and f1 are standard metrics.\n",
        "metric = evaluate.load(\"glue\", \"mrpc\") # mrpc provides accuracy and f1, which are generally useful\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "print(\"Evaluation metrics defined (accuracy, f1).\")\n",
        "\n",
        "# 3. Configure training parameters with TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",             # output directory\n",
        "    num_train_epochs=3,                 # total number of training epochs\n",
        "    per_device_train_batch_size=16,     # batch size per device during training\n",
        "    per_device_eval_batch_size=16,      # batch size for evaluation\n",
        "    warmup_steps=500,                   # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,                  # strength of weight decay\n",
        "    logging_dir=\"./logs\",               # directory for storing logs\n",
        "    logging_steps=100,                  # log every x updates steps\n",
        "    eval_strategy=\"epoch\",        # evaluate each epoch\n",
        "    save_strategy=\"epoch\",              # save checkpoint each epoch\n",
        "    load_best_model_at_end=True,        # load the best model when training ends\n",
        "    metric_for_best_model=\"accuracy\",   # use accuracy to determine the best model\n",
        "    report_to=\"none\"                    # Disable integrations for simplicity\n",
        ")\n",
        "\n",
        "print(\"Training arguments configured.\")\n",
        "\n",
        "# Display some training arguments for verification\n",
        "print(f\"\\nTraining Arguments Summary:\")\n",
        "print(f\"  Number of epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"  Train batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"  Evaluation strategy: {training_args.eval_strategy}\")\n",
        "\n",
        "# Initialize DataCollatorWithPadding\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_test_dataset,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a3b636e"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block successfully initialized the `Trainer` object. Now, as per the task requirements, the next logical step is to start the model training using the initialized `trainer`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "708da131"
      },
      "source": [
        "print(\"Starting model training...\")\n",
        "trainer.train()\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# Evaluate the final model after training\n",
        "print(\"Evaluating the trained model on the test set...\")\n",
        "metrics = trainer.evaluate()\n",
        "print(f\"Final evaluation metrics: {metrics}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2df867b6"
      },
      "source": [
        "# Task\n",
        "Okay, I will proceed with the next steps.\n",
        "\n",
        "**Saving and Loading Model and Tokenizer**: I will save the trained model (including its state and tokenizer) and demonstrate how to reload it for subsequent use, such as inference. This ensures model persistence.\n",
        "**Model Inference Example**: Using the loaded model, I will predict on new, unseen text samples, show how the model performs the actual text classification task, and decode its output. This will verify the model's generalization capabilities.\n",
        "**Finish task**: I will conclude the entire SLM training and testing process, and provide final insights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfd14372"
      },
      "source": [
        "## 保存和加载模型\n",
        "\n",
        "### Subtask:\n",
        "保存训练好的模型（包括模型状态和tokenizer），并演示如何重新加载它以备后续使用，例如进行推理。这将确保模型的持久性。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b52223c3"
      },
      "source": [
        "### 模型推理示例\n",
        "\n",
        "我们将使用之前保存并重新加载的模型 (`loaded_model`) 和分词器 (`loaded_tokenizer`) 对新的文本样本进行情感分类预测。这演示了模型如何在新数据上进行实际的推理任务。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50cb5824"
      },
      "source": [
        "import os\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "# 1. Define a directory to save the model and tokenizer\n",
        "save_directory = \"./my_slm_model\"\n",
        "os.makedirs(save_directory, exist_ok=True)\n",
        "\n",
        "print(f\"Saving model and tokenizer to {save_directory}...\")\n",
        "# 2. Use trainer.save_model() to save the entire model and tokenizer\n",
        "trainer.save_model(save_directory)\n",
        "print(\"Model and tokenizer saved successfully.\")\n",
        "\n",
        "# 3. Reload the model from the saved directory\n",
        "print(f\"Reloading model from {save_directory}...\")\n",
        "loaded_model = AutoModelForSequenceClassification.from_pretrained(save_directory)\n",
        "print(\"Model reloaded successfully.\")\n",
        "\n",
        "# 4. Reload the tokenizer from the saved directory\n",
        "print(f\"Reloading tokenizer from {save_directory}...\")\n",
        "loaded_tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
        "print(\"Tokenizer reloaded successfully.\")\n",
        "\n",
        "# 5. Print confirmation\n",
        "print(\"Confirmation: The model and tokenizer have been successfully saved and reloaded.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27b30779"
      },
      "source": [
        "import torch\n",
        "\n",
        "# 1. Prepare new text samples for inference\n",
        "new_texts = [\n",
        "    \"This movie was absolutely fantastic! I loved every single moment of it.\",\n",
        "    \"What a terrible film. I wasted two hours of my life watching this trash.\",\n",
        "    \"The plot was a bit confusing, but the acting was superb.\"\n",
        "]\n",
        "\n",
        "# 2. Tokenize the new texts using the reloaded tokenizer\n",
        "print(\"Tokenizing new texts...\")\n",
        "inputs = loaded_tokenizer(new_texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "# Move inputs to GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
        "    loaded_model.to('cuda')\n",
        "\n",
        "# 3. Perform inference with the reloaded model\n",
        "print(\"Performing inference...\")\n",
        "loaded_model.eval() # Set model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    outputs = loaded_model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "# 4. Decode the predictions\n",
        "print(\"Decoding predictions...\")\n",
        "label_map = {0: \"Negative\", 1: \"Positive\"} # Based on imdb dataset labels\n",
        "\n",
        "for i, text in enumerate(new_texts):\n",
        "    predicted_label = label_map[predictions[i].item()]\n",
        "    print(f\"\\nText: '{text}'\")\n",
        "    print(f\"Predicted Sentiment: {predicted_label}\")\n",
        "\n",
        "print(\"Model inference demonstration complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57eaf740"
      },
      "source": [
        "## 定义对话生成任务\n",
        "\n",
        "### Subtask:\n",
        "明确定义小型语言模型（SLM）在对话生成任务中的目标和应用场景，例如基于给定提示生成连贯且有意义的回应。\n",
        "\n",
        "#### Instructions\n",
        "1.  **确定角色**：您的SLM将在对话生成任务中扮演什么角色？例如，它是作为一个通用聊天机器人、一个内容创作助手（如撰写营销文案、诗歌），还是一个特定领域的问答系统（如客服机器人、医疗咨询助手）？\n",
        "2.  **明确具体目标**：您的SLM在对话生成中的具体目标是什么？\n",
        "    *   **流畅性与自然度**：生成的回应是否需要像人类对话一样流畅自然？\n",
        "    *   **信息性与准确性**：回应是否需要提供准确的信息，还是更侧重于创造性或娱乐性？\n",
        "    *   **连贯性**：在多轮对话中，模型是否需要保持主题一致性和上下文连贯性？\n",
        "    *   **语气与风格**：是否需要遵循特定的语气（如正式、非正式、幽默）或风格？\n",
        "    *   **多样性**：生成的回应是需要多样化，还是可以有重复模式？\n",
        "3.  **识别应用场景**：考虑资源限制（如边缘设备、移动应用、嵌入式系统）和性能需求（如实时响应、低延迟、离线可用性）。什么样的对话生成应用场景最适合SLM？\n",
        "4.  **撰写定义**：基于上述分析，为您的SLM对话生成任务撰写一个简短而清晰的定义，包括其核心目标、预期功能和主要的适用场景。\n",
        "\n",
        "**示例（通用聊天机器人场景）**：\n",
        "\"我们的SLM将作为一个轻量级的通用聊天机器人，其核心目标是基于用户输入的提示，生成流畅、自然且具备基本常识的单轮对话回应。它应能理解常见问题和意图，并提供简短、友好的答复。主要应用场景包括移动应用内的用户辅助、简单客服问答以及作为离线设备上的互动伴侣。\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "473f7c52"
      },
      "source": [
        "## 对话数据集准备\n",
        "\n",
        "### Subtask:\n",
        "选择并加载一个适合对话生成的公开数据集（例如，Hugging Face `datasets`库中较小的对话或问答数据集）。对数据进行预处理，包括格式化对话轮次（例如：`user: ...\\nagent: ...`）和使用适合生成式模型的`AutoTokenizer`进行分词，并转换为PyTorch `Dataset`格式。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "635ed299"
      },
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# 1. Load a suitable public dataset for dialogue generation (e.g., HuggingFaceH4/ultrachat_200k)\n",
        "print(\"Loading HuggingFaceH4/ultrachat_200k dataset...\")\n",
        "dataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\")\n",
        "\n",
        "# ultrachat_200k has 'train_sft' and 'test_sft' splits\n",
        "train_dataset = dataset[\"train_sft\"]\n",
        "test_dataset = dataset[\"test_sft\"]\n",
        "\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "print(\"Example from train_dataset (original format):\")\n",
        "first_example = train_dataset[0]\n",
        "print(first_example) # Print the entire first example to understand its actual structure and keys\n",
        "\n",
        "# Get available column names\n",
        "train_column_names = train_dataset.column_names\n",
        "print(f\"Train dataset column names: {train_column_names}\")\n",
        "\n",
        "# 2. Initialize an AutoTokenizer suitable for a generative model (e.g., gpt2)\n",
        "model_name_generative = \"gpt2\"\n",
        "tokenizer_generative = AutoTokenizer.from_pretrained(model_name_generative)\n",
        "\n",
        "# GPT-2 tokenizer does not have a pad token by default, which is needed for batching.\n",
        "# We set it to the eos_token, which is common practice for GPT-like models in generation tasks.\n",
        "if tokenizer_generative.pad_token is None:\n",
        "    tokenizer_generative.pad_token = tokenizer_generative.eos_token\n",
        "\n",
        "print(f\"Generative Tokenizer loaded: {model_name_generative}\")\n",
        "print(f\"Pad token set to: {tokenizer_generative.pad_token}\")\n",
        "\n",
        "# 3. Define a preprocessing function to format dialogue turns and tokenize\n",
        "def format_and_tokenize_dialogue(examples):\n",
        "    formatted_dialogues = []\n",
        "    # Corrected: Use 'messages' column for conversations\n",
        "    for messages_list in examples[\"messages\"]:\n",
        "        formatted_utterances = []\n",
        "        for message in messages_list:\n",
        "            role_translated = \"用户\" if message[\"role\"] == \"user\" else \"代理\"\n",
        "            formatted_utterances.append(f\"{role_translated}: {message['content'].strip()}\")\n",
        "        formatted_dialogues.append(\"\\n\".join(formatted_utterances) + tokenizer_generative.eos_token)\n",
        "\n",
        "    return tokenizer_generative(formatted_dialogues, truncation=True, max_length=256, padding=\"max_length\")\n",
        "\n",
        "# 4. Apply the preprocessing function to the datasets\n",
        "print(\"Formatting and tokenizing train dataset...\")\n",
        "# Remove original columns to only keep tokenized inputs\n",
        "tokenized_train_dialogue_dataset = train_dataset.map(\n",
        "    format_and_tokenize_dialogue, batched=True, remove_columns=train_dataset.column_names\n",
        ")\n",
        "print(\"Formatting and tokenizing test dataset...\")\n",
        "tokenized_test_dialogue_dataset = test_dataset.map(\n",
        "    format_and_tokenize_dialogue, batched=True, remove_columns=test_dataset.column_names\n",
        ")\n",
        "\n",
        "print(\"Example from tokenized_train_dialogue_dataset (after processing):\")\n",
        "print(tokenized_train_dialogue_dataset[0])\n",
        "\n",
        "# 5. Set dataset format to PyTorch 'torch'\n",
        "print(\"Setting dataset format to torch...\")\n",
        "tokenized_train_dialogue_dataset.set_format(\"torch\")\n",
        "tokenized_test_dialogue_dataset.set_format(\"torch\")\n",
        "\n",
        "print(\"Dialogue data preparation and tokenization complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11f23def"
      },
      "source": [
        "## 生成式模型架构选择与加载\n",
        "\n",
        "### Subtask:\n",
        "选择一个适合对话生成的SLM架构，例如基于Transformer的仅解码器模型（如`distilgpt2`或`gpt2-small`）。加载相应的`AutoModelForCausalLM`，该模型专注于自回归生成文本。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbd8d0fe"
      },
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Mount Google Drive to access the saved model\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Define the paths\n",
        "# Path in Google Drive (based on your previous backup)\n",
        "drive_save_path = \"/content/drive/My Drive/SLM_Project_Backup/my_chat_slm\"\n",
        "# Local path\n",
        "local_save_path = \"./my_chat_slm\"\n",
        "\n",
        "# 3. Logic to select the model path\n",
        "if os.path.exists(drive_save_path):\n",
        "    model_name_generative = drive_save_path\n",
        "    print(f\"Found model in Google Drive. Loading from '{drive_save_path}' to continue training...\")\n",
        "elif os.path.exists(local_save_path):\n",
        "    model_name_generative = local_save_path\n",
        "    print(f\"Found locally saved model. Loading from '{local_save_path}' to continue training...\")\n",
        "else:\n",
        "    model_name_generative = \"gpt2\"\n",
        "    print(f\"Loading base model '{model_name_generative}'...\")\n",
        "\n",
        "# 4. Load the corresponding AutoModelForCausalLM\n",
        "print(f\"Loading generative model: {model_name_generative}...\")\n",
        "generative_model = AutoModelForCausalLM.from_pretrained(model_name_generative)\n",
        "\n",
        "print(f\"Generative model loaded successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cbf6bf3"
      },
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "# 1. Configure training parameters with TrainingArguments\n",
        "# Adjust parameters suitable for generative models and the ultrachat_200k dataset\n",
        "training_args_generative = TrainingArguments(\n",
        "    output_dir=\"./generative_results\",         # output directory for the generative model\n",
        "    num_train_epochs=3,                         # INCREASED to 3 for better convergence (was 1)\n",
        "    per_device_train_batch_size=8,             # smaller batch size due to longer sequence length\n",
        "    per_device_eval_batch_size=8,              # batch size for evaluation\n",
        "    warmup_steps=500,                           # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,                          # strength of weight decay\n",
        "    logging_dir=\"./generative_logs\",           # directory for storing logs\n",
        "    logging_steps=100,                          # log every x updates steps\n",
        "    eval_strategy=\"epoch\",                      # evaluate each epoch\n",
        "    save_strategy=\"epoch\",                      # save checkpoint each epoch\n",
        "    load_best_model_at_end=True,                # load the best model when training ends\n",
        "    metric_for_best_model=\"eval_loss\",          # use evaluation loss to determine the best model for generation\n",
        "    report_to=\"none\",                           # Disable integrations for simplicity\n",
        "    gradient_accumulation_steps=2,              # Accumulate gradients to simulate larger batch size\n",
        "    fp16=True                                   # Enable mixed precision training\n",
        ")\n",
        "\n",
        "print(\"Generative Training arguments configured.\")\n",
        "\n",
        "# Display some training arguments for verification\n",
        "print(f\"\\nGenerative Training Arguments Summary:\")\n",
        "print(f\"  Number of epochs: {training_args_generative.num_train_epochs}\")\n",
        "print(f\"  Train batch size: {training_args_generative.per_device_train_batch_size}\")\n",
        "print(f\"  Evaluation strategy: {training_args_generative.eval_strategy}\")\n",
        "\n",
        "# 2. Initialize DataCollatorForLanguageModeling\n",
        "# This collator will dynamically pad sequences to the longest length in a batch\n",
        "# and also shift labels for causal language modeling.\n",
        "data_collator_generative = DataCollatorForLanguageModeling(tokenizer=tokenizer_generative, mlm=False)\n",
        "\n",
        "print(\"DataCollatorForLanguageModeling initialized.\")\n",
        "\n",
        "# 3. Initialize Trainer\n",
        "trainer_generative = Trainer(\n",
        "    model=generative_model,\n",
        "    args=training_args_generative,\n",
        "    train_dataset=tokenized_train_dialogue_dataset,\n",
        "    eval_dataset=tokenized_test_dialogue_dataset,\n",
        "    data_collator=data_collator_generative\n",
        ")\n",
        "\n",
        "print(\"Generative Trainer initialized.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd152314"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block successfully initialized the `Trainer` for the generative model. The next step is to initiate the training process for the dialogue generation model using the `trainer_generative.train()` method.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d57c0399"
      },
      "source": [
        "import os\n",
        "\n",
        "print(\"Starting generative model training...\")\n",
        "\n",
        "# Check for checkpoints to resume\n",
        "# This allows continuing training if interrupted or extending epochs\n",
        "output_dir = \"./generative_results\"\n",
        "resume_from_checkpoint = False\n",
        "if os.path.exists(output_dir) and len([x for x in os.listdir(output_dir) if \"checkpoint\" in x]) > 0:\n",
        "    resume_from_checkpoint = True\n",
        "    print(\"Found checkpoints. Resuming training from the latest checkpoint...\")\n",
        "else:\n",
        "    print(\"No checkpoints found. Starting training sequence...\")\n",
        "\n",
        "trainer_generative.train(resume_from_checkpoint=resume_from_checkpoint)\n",
        "print(\"Generative model training complete.\")\n",
        "\n",
        "# Evaluate the final generative model after training\n",
        "print(\"Evaluating the trained generative model on the test set...\")\n",
        "metrics_generative = trainer_generative.evaluate()\n",
        "print(f\"Final generative evaluation metrics: {metrics_generative}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce52a1cd"
      },
      "source": [
        "## 保存与测试生成式模型\n",
        "\n",
        "### Subtask:\n",
        "保存训练好的对话生成模型，并使用它进行简单的对话推理，以验证其生成能力。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "991bc4b5"
      },
      "source": [
        "import os\n",
        "\n",
        "# 1. Define save directory for the generative model\n",
        "save_directory_gen = \"./my_chat_slm\"\n",
        "os.makedirs(save_directory_gen, exist_ok=True)\n",
        "\n",
        "print(f\"Saving generative model and tokenizer to {save_directory_gen}...\")\n",
        "# 2. Save the model and tokenizer\n",
        "trainer_generative.save_model(save_directory_gen)\n",
        "tokenizer_generative.save_pretrained(save_directory_gen)\n",
        "print(\"Generative model saved successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d510e5bf"
      },
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "# Ensure save_directory_gen is defined\n",
        "try:\n",
        "    save_directory_gen\n",
        "except NameError:\n",
        "    save_directory_gen = \"./my_chat_slm\"\n",
        "\n",
        "# 1. Load the saved model\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(f\"Loading model from {save_directory_gen}...\")\n",
        "chat_pipeline = pipeline(\"text-generation\", model=save_directory_gen, tokenizer=save_directory_gen, device=device)\n",
        "\n",
        "print(\"Chat pipeline loaded. Ready for interaction!\")\n",
        "\n",
        "# 2. Define a function to generate responses\n",
        "def generate_response(prompt_text):\n",
        "    # Format the prompt\n",
        "    formatted_prompt = f\"User: {prompt_text}\\nAgent:\"\n",
        "\n",
        "    sequences = chat_pipeline(\n",
        "        formatted_prompt,\n",
        "        max_new_tokens=64,   # Change: Generate up to 64 NEW tokens (ignoring prompt length)\n",
        "        num_return_sequences=1,\n",
        "        pad_token_id=chat_pipeline.tokenizer.eos_token_id,\n",
        "        truncation=True,\n",
        "        do_sample=True,\n",
        "        top_k=40,       # Slightly reduced for stability\n",
        "        top_p=0.9,\n",
        "        temperature=0.6, # Slightly reduced randomness for more focused answers\n",
        "        repetition_penalty=1.2 # Prevent repeating the same phrases\n",
        "    )\n",
        "\n",
        "    # Extract the generated text\n",
        "    generated_text = sequences[0]['generated_text']\n",
        "\n",
        "    # Clean up output\n",
        "    try:\n",
        "        # Get text after the *last* \"Agent:\"\n",
        "        response = generated_text.split(\"Agent:\")[-1].strip()\n",
        "        # Cut off if it generates a new \"User:\" turn\n",
        "        response = response.split(\"User:\")[0].strip()\n",
        "    except IndexError:\n",
        "        response = generated_text\n",
        "\n",
        "    return response\n",
        "\n",
        "# 3. Interactive Chat Loop\n",
        "print(\"--- Interactive Chat SLM (Type 'exit' to quit) ---\")\n",
        "while True:\n",
        "    try:\n",
        "        user_input = input(\"User: \")\n",
        "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "            print(\"Exiting chat...\")\n",
        "            break\n",
        "\n",
        "        if not user_input.strip():\n",
        "            continue\n",
        "\n",
        "        response = generate_response(user_input)\n",
        "        print(f\"Agent: {response}\\n\")\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nExiting chat...\")\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 保存drive"
      ],
      "metadata": {
        "id": "2FSSnM4-Xyof"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "575f6a7a"
      },
      "source": [
        "import shutil\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "# This will pop up a window asking for permission\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Define source and destination paths\n",
        "# The folders we want to backup\n",
        "source_folders = [\"./my_slm_model\", \"./my_chat_slm\"]\n",
        "\n",
        "# The destination folder in Google Drive\n",
        "# We'll create a folder named 'SLM_Project_Backup'\n",
        "destination_root = \"/content/drive/My Drive/SLM_Project_Backup\"\n",
        "\n",
        "# 3. Copy folders\n",
        "for folder in source_folders:\n",
        "    if os.path.exists(folder):\n",
        "        # Construct full destination path\n",
        "        folder_name = os.path.basename(folder)\n",
        "        dest_path = os.path.join(destination_root, folder_name)\n",
        "\n",
        "        print(f\"Backing up '{folder}' to '{dest_path}'...\")\n",
        "\n",
        "        # Remove destination if it exists (to ensure fresh copy)\n",
        "        if os.path.exists(dest_path):\n",
        "            shutil.rmtree(dest_path)\n",
        "\n",
        "        # Copy the directory tree\n",
        "        shutil.copytree(folder, dest_path)\n",
        "        print(f\"Successfully backed up '{folder_name}'.\")\n",
        "    else:\n",
        "        print(f\"Warning: Source folder '{folder}' does not exist. Skipping.\")\n",
        "\n",
        "print(\"\\nBackup to Google Drive complete!\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}